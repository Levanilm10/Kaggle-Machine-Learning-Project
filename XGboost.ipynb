{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5292d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "587c072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f166e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = gpd.read_file('train.geojson')\n",
    "test_df = gpd.read_file('test.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fa6c",
   "metadata": {},
   "source": [
    "# <u> Data Preprocessing and Cleaning <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['change_type'].map(change_type_map) #we convert the target variable to numeric values using the dictionary we created earlier\n",
    "x_train=train_df.drop(columns=['change_type','index'])#we drop index and change_type columns from the training data\n",
    "x_test=test_df.drop(columns=['index'])#we drop index column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3df0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee766b",
   "metadata": {},
   "source": [
    "### Geometry features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c87ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_features(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "\n",
    "    gdf_m = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    area = gdf_m.geometry.area\n",
    "    perim = gdf_m.geometry.length\n",
    "\n",
    "\n",
    "    gdf_ll = gdf.to_crs(epsg=4326)\n",
    "    cent = gdf_ll.geometry.centroid\n",
    "    bounds = gdf_ll.geometry.bounds  \n",
    "\n",
    "    gdf[\"geom_area_m2\"] = area\n",
    "    gdf[\"geom_perim_m\"] = perim\n",
    "    gdf[\"geom_compactness\"] = (4 * np.pi * area) / (perim**2 + 1e-9)\n",
    "\n",
    "    gdf[\"centroid_lon\"] = cent.x\n",
    "    gdf[\"centroid_lat\"] = cent.y\n",
    "\n",
    "    gdf[\"bbox_width_deg\"]  = bounds[\"maxx\"] - bounds[\"minx\"]\n",
    "    gdf[\"bbox_height_deg\"] = bounds[\"maxy\"] - bounds[\"miny\"]\n",
    "    gdf[\"bbox_area_deg2\"]  = gdf[\"bbox_width_deg\"] * gdf[\"bbox_height_deg\"]\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc06c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x_train\n",
    "x_train = add_geometry_features(x_train)\n",
    "x_train = x_train.drop(columns=[\"geometry\"])\n",
    "\n",
    "# for x_test\n",
    "x_test = add_geometry_features(x_test)\n",
    "x_test = x_test.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ff2e6",
   "metadata": {},
   "source": [
    "### Date features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _detect_date_indices(df):\n",
    "    date_cols = [c for c in df.columns if re.fullmatch(r\"date\\d+\", c)]\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"Aucune colonne dateX trouv√©e (date0.. ou date1..)\")\n",
    "    idxs = sorted([int(c.replace(\"date\",\"\")) for c in date_cols])\n",
    "    return idxs, [f\"date{i}\" for i in idxs]\n",
    "\n",
    "def add_temporal_status_image_features(df: pd.DataFrame, drop_raw: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Trie chaque ligne par les vraies dates (NaT √† la fin)\n",
    "    - Construit des features de statuts (ordre + transitions)\n",
    "    - Construit des features RGB (first/last/deltas) apr√®s tri\n",
    "    - Ajoute aussi date_span_days + deltas entre dates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    idxs, date_cols = _detect_date_indices(df)\n",
    "    k = len(date_cols)\n",
    "\n",
    "    status_cols = [f\"change_status_date{i}\" for i in idxs if f\"change_status_date{i}\" in df.columns]\n",
    "    if len(status_cols) != k:\n",
    "\n",
    "        status_cols = [c for c in df.columns if re.fullmatch(r\"change_status_date\\d+\", c)]\n",
    "        status_idxs = sorted([int(c.split(\"date\")[-1]) for c in status_cols])\n",
    "        status_cols = [f\"change_status_date{i}\" for i in status_idxs]\n",
    "        k = min(len(date_cols), len(status_cols))\n",
    "        date_cols, status_cols, idxs = date_cols[:k], status_cols[:k], idxs[:k]\n",
    "\n",
    "\n",
    "    img_specs = []\n",
    "    for color in [\"red\", \"green\", \"blue\"]:\n",
    "        for stat in [\"mean\", \"std\"]:\n",
    "            cols = []\n",
    "            ok = True\n",
    "            for i in idxs[:k]:\n",
    "                c = f\"img_{color}_{stat}_date{i}\"\n",
    "                if c in df.columns:\n",
    "                    cols.append(c)\n",
    "                else:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                img_specs.append((color, stat, cols))\n",
    "\n",
    "\n",
    "    D = df[date_cols].apply(pd.to_datetime, dayfirst=True, errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    Di = D.astype(\"int64\")\n",
    "    nat_mask = (Di == np.iinfo(np.int64).min)\n",
    "    Di[nat_mask] = np.iinfo(np.int64).max\n",
    "\n",
    "    order = np.argsort(Di, axis=1)\n",
    "    Di_sorted = np.take_along_axis(Di, order, axis=1)\n",
    "\n",
    "    maxv = np.iinfo(np.int64).max\n",
    "    valid_mask = (Di_sorted != maxv)\n",
    "    last_idx = valid_mask.sum(axis=1) - 1\n",
    "\n",
    "\n",
    "    span_days = np.full(len(df), np.nan)\n",
    "    ok = last_idx >= 0\n",
    "    span_days[ok] = (Di_sorted[ok, last_idx[ok]] - Di_sorted[ok, 0]) / (24 * 3600 * 1e9)\n",
    "    df[\"date_span_days\"] = span_days\n",
    "\n",
    "    for j in range(k - 1):\n",
    "        a, b = Di_sorted[:, j], Di_sorted[:, j + 1]\n",
    "        delta = (b - a) / (24 * 3600 * 1e9)\n",
    "        bad = (a == maxv) | (b == maxv)\n",
    "        delta[bad] = np.nan\n",
    "        df[f\"delta_sorted_{j}_{j+1}_days\"] = delta\n",
    "\n",
    "\n",
    "    S = df[status_cols].astype(object).to_numpy()\n",
    "    S_sorted = np.take_along_axis(S, order[:, :k], axis=1)\n",
    "\n",
    "\n",
    "    Sdf = pd.DataFrame(S_sorted)\n",
    "    df[\"status_first\"] = Sdf.bfill(axis=1).iloc[:, 0]\n",
    "    df[\"status_last\"]  = Sdf.ffill(axis=1).iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_first\"], prefix=\"status_first\", dummy_na=True)], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_last\"],  prefix=\"status_last\",  dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    level_map = {\n",
    "        \"Land Cleared\": 0,\n",
    "        \"Construction Started\": 1,\n",
    "        \"Materials Dumped\": 1,\n",
    "        \"Construction Midway\": 2,\n",
    "        \"Construction Done\": 3,\n",
    "        \"Prior Construction\": 3,\n",
    "        \"Operational\": 4,\n",
    "    }\n",
    "\n",
    "    L = np.full((len(df), k), np.nan)\n",
    "    for j in range(k):\n",
    "        L[:, j] = pd.Series(S_sorted[:, j]).map(level_map).to_numpy(dtype=float)\n",
    "        df[f\"status_level_sorted_{j}\"] = L[:, j]\n",
    "\n",
    "    Ldf = pd.DataFrame(L)\n",
    "    first_level = Ldf.bfill(axis=1).iloc[:, 0].to_numpy()\n",
    "    last_level  = Ldf.ffill(axis=1).iloc[:, -1].to_numpy()\n",
    "\n",
    "    df[\"status_level_first\"] = first_level\n",
    "    df[\"status_level_last\"]  = last_level\n",
    "    df[\"status_progress\"]    = last_level - first_level\n",
    "    df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
    "    df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
    "\n",
    "\n",
    "    mask = ~pd.DataFrame(S_sorted).isna().to_numpy()\n",
    "    n_changes = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        m = mask[:, j] & mask[:, j + 1]\n",
    "        n_changes += (m & (S_sorted[:, j] != S_sorted[:, j + 1]))\n",
    "    df[\"status_n_changes\"] = n_changes\n",
    "\n",
    "    n_reg = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        a, b = L[:, j], L[:, j + 1]\n",
    "        m = np.isfinite(a) & np.isfinite(b)\n",
    "        n_reg += (m & (b < a))\n",
    "    df[\"status_n_regressions\"] = n_reg\n",
    "    df[\"status_has_regression\"] = (n_reg > 0).astype(int)\n",
    "\n",
    "\n",
    "    first_date = Di_sorted[:, 0]\n",
    "    time_to_done = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)):\n",
    "        if first_date[i] == maxv:\n",
    "            continue\n",
    "        idx_done = None\n",
    "        for j in range(k):\n",
    "            if valid_mask[i, j] and np.isfinite(L[i, j]) and L[i, j] >= 3:\n",
    "                idx_done = j\n",
    "                break\n",
    "        if idx_done is not None:\n",
    "            time_to_done[i] = (Di_sorted[i, idx_done] - first_date[i]) / (24 * 3600 * 1e9)\n",
    "    df[\"time_to_done_days\"] = time_to_done\n",
    "\n",
    "\n",
    "    for color, stat, cols in img_specs:\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        Xs = np.take_along_axis(X, order[:, :k], axis=1)\n",
    "        Xs = Xs.copy()\n",
    "        Xs[~valid_mask] = np.nan\n",
    "\n",
    "        first = Xs[:, 0]\n",
    "        last = np.full(len(df), np.nan)\n",
    "        ok = last_idx >= 0\n",
    "        last[ok] = Xs[ok, last_idx[ok]]\n",
    "\n",
    "        df[f\"img_{color}_{stat}_first\"] = first\n",
    "        df[f\"img_{color}_{stat}_last\"]  = last\n",
    "        df[f\"img_{color}_{stat}_delta\"] = last - first\n",
    "\n",
    "        for j in range(k - 1):\n",
    "            df[f\"img_{color}_{stat}_delta_{j}_{j+1}\"] = Xs[:, j + 1] - Xs[:, j]\n",
    "\n",
    "\n",
    "    if drop_raw:\n",
    "        drop_cols = []\n",
    "        drop_cols += date_cols + status_cols\n",
    "        for _, _, cols in img_specs:\n",
    "            drop_cols += cols\n",
    "        drop_cols += [\"status_first\", \"status_last\"]\n",
    "        df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create temporal features for both train and test\n",
    "x_train = add_temporal_status_image_features(x_train)\n",
    "x_test = add_temporal_status_image_features(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4ef1e",
   "metadata": {},
   "source": [
    "### Multi-hot encoding: urban_type & geography_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyse des valeurs:\n",
      "urban_type - Train: 5, Test: 5, Total: 5\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n",
      "\n",
      "geography_type - Train: 11, Test: 11, Total: 11\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n"
     ]
    }
   ],
   "source": [
    "def multi_hot_encode(df: pd.DataFrame, column: str, all_values: set = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Cr√©e un encodage multi-hot pour une colonne avec des valeurs s√©par√©es par des virgules.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec la colonne √† encoder\n",
    "        column: Nom de la colonne √† encoder\n",
    "        all_values: Ensemble des valeurs possibles (optionnel). Si None, extrait du df.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame encod√©, set des valeurs uniques)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    if all_values is None:\n",
    "        all_values = set()\n",
    "        for val in df[column].dropna():\n",
    "            if pd.notna(val) and val != 'N,A': \n",
    "\n",
    "                values = [v.strip() for v in str(val).split(',')]\n",
    "                all_values.update(values)\n",
    "    \n",
    "\n",
    "    for value in sorted(all_values):\n",
    "\n",
    "        col_name = f\"{column}_{value.replace(' ', '_').replace('/', '_')}\"\n",
    "        \n",
    "\n",
    "        df[col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x).split(',') else 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df, all_values\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_values(df: pd.DataFrame, column: str) -> set:\n",
    "    \"\"\"Extrait toutes les valeurs uniques d'une colonne.\"\"\"\n",
    "    all_values = set()\n",
    "    for val in df[column].dropna():\n",
    "        if pd.notna(val) and val != 'N,A':\n",
    "            values = [v.strip() for v in str(val).split(',')]\n",
    "            all_values.update(values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "urban_values_train = extract_all_values(x_train, 'urban_type')\n",
    "urban_values_test = extract_all_values(test_df, 'urban_type')\n",
    "urban_values_all = urban_values_train | urban_values_test  \n",
    "\n",
    "geo_values_train = extract_all_values(x_train, 'geography_type')\n",
    "geo_values_test = extract_all_values(test_df, 'geography_type')\n",
    "geo_values_all = geo_values_train | geo_values_test\n",
    "\n",
    "print(\"üîç Analyse des valeurs:\")\n",
    "print(f\"urban_type - Train: {len(urban_values_train)}, Test: {len(urban_values_test)}, Total: {len(urban_values_all)}\")\n",
    "print(f\"  Uniquement dans train: {urban_values_train - urban_values_test}\")\n",
    "print(f\"  Uniquement dans test: {urban_values_test - urban_values_train}\")\n",
    "\n",
    "print(f\"\\ngeography_type - Train: {len(geo_values_train)}, Test: {len(geo_values_test)}, Total: {len(geo_values_all)}\")\n",
    "print(f\"  Uniquement dans train: {geo_values_train - geo_values_train.intersection(geo_values_test) if len(geo_values_train - geo_values_test) <= 5 else '(trop pour afficher)'}\")\n",
    "print(f\"  Uniquement dans test: {geo_values_test - geo_values_test.intersection(geo_values_train) if len(geo_values_test - geo_values_train) <= 5 else '(trop pour afficher)'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3301c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296146, 95)\n",
      "Test shape: (120526, 95)\n",
      "Test shape: (120526, 95)\n"
     ]
    }
   ],
   "source": [
    "#multi-hot encoding on train\n",
    "x_train, _ = multi_hot_encode(x_train, 'urban_type', urban_values_all)\n",
    "x_train, _ = multi_hot_encode(x_train, 'geography_type', geo_values_all)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "\n",
    "#multi-hot encoding on test\n",
    "x_test, _ = multi_hot_encode(x_test, 'urban_type', urban_values_all)\n",
    "x_test, _ = multi_hot_encode(x_test, 'geography_type', geo_values_all)\n",
    "print(f\"Test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0e146",
   "metadata": {},
   "source": [
    "### Drop additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8c664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"delta_sorted_0_1_days\",\"delta_sorted_1_2_days\", \"delta_sorted_2_3_days\", \"delta_sorted_3_4_days\",\"index\"]\n",
    "\n",
    "\n",
    "cols_to_drop_train = [col for col in cols_to_drop if col in x_train.columns]\n",
    "cols_to_drop_test = [col for col in cols_to_drop if col in x_test.columns]\n",
    "\n",
    "x_train = x_train.drop(columns=cols_to_drop_train)\n",
    "x_test = x_test.drop(columns=cols_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec455e",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inf_count_train = np.isinf(x_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_count_test = np.isinf(x_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Train: {inf_count_train} valeurs infinies\")\n",
    "print(f\"Test: {inf_count_test} valeurs infinies\")\n",
    "\n",
    "\n",
    "x_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
    "x_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_final = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols_final = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_train[numerical_cols_final] = numerical_imputer.fit_transform(x_train[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_train[categorical_cols_final] = categorical_imputer.fit_transform(x_train[categorical_cols_final])\n",
    "\n",
    "\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_test[numerical_cols_final] = numerical_imputer.transform(x_test[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_test[categorical_cols_final] = categorical_imputer.transform(x_test[categorical_cols_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c49bd",
   "metadata": {},
   "source": [
    "# <u> LightGBM Modele training <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b057167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a7048",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 1/3\n",
      "============================================================\n",
      "Train macro F1:    0.8740\n",
      "Train weighted F1: 0.8289\n",
      "Val macro F1:      0.5570\n",
      "Val weighted F1:   0.7677\n",
      "\n",
      "============================================================\n",
      "Fold 2/3\n",
      "============================================================\n",
      "Train macro F1:    0.8740\n",
      "Train weighted F1: 0.8289\n",
      "Val macro F1:      0.5570\n",
      "Val weighted F1:   0.7677\n",
      "\n",
      "============================================================\n",
      "Fold 2/3\n",
      "============================================================\n",
      "Train macro F1:    0.8719\n",
      "Train weighted F1: 0.8298\n",
      "Val macro F1:      0.5551\n",
      "Val weighted F1:   0.7655\n",
      "\n",
      "============================================================\n",
      "Fold 3/3\n",
      "============================================================\n",
      "Train macro F1:    0.8719\n",
      "Train weighted F1: 0.8298\n",
      "Val macro F1:      0.5551\n",
      "Val weighted F1:   0.7655\n",
      "\n",
      "============================================================\n",
      "Fold 3/3\n",
      "============================================================\n",
      "Train macro F1:    0.8754\n",
      "Train weighted F1: 0.8304\n",
      "Val macro F1:      0.5585\n",
      "Val weighted F1:   0.7663\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "Train macro   = 0.8738 ¬± 0.0014\n",
      "Train weighted= 0.8297 ¬± 0.0006\n",
      "Val   macro   = 0.5569 ¬± 0.0014\n",
      "Val   weighted= 0.7665 ¬± 0.0009\n",
      "Train macro F1:    0.8754\n",
      "Train weighted F1: 0.8304\n",
      "Val macro F1:      0.5585\n",
      "Val weighted F1:   0.7663\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "Train macro   = 0.8738 ¬± 0.0014\n",
      "Train weighted= 0.8297 ¬± 0.0006\n",
      "Val   macro   = 0.5569 ¬± 0.0014\n",
      "Val   weighted= 0.7665 ¬± 0.0009\n"
     ]
    }
   ],
   "source": [
    "X = x_train.values\n",
    "y = y_train.values\n",
    "\n",
    "classes_global = np.unique(y)\n",
    "num_class = len(classes_global)\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "\n",
    "tr_macro_list, tr_weight_list = [], []\n",
    "va_macro_list, va_weight_list = [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y), start=1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/3\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "\n",
    "    w = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_tr), y=y_tr)\n",
    "    sample_weights = np.array([w[int(label)] for label in y_tr])\n",
    "\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=num_class,\n",
    "        n_estimators=1500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=100,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_va = model.predict(X_va)\n",
    "\n",
    "    tr_macro = f1_score(y_tr, pred_tr, average=\"macro\")\n",
    "    tr_weight = f1_score(y_tr, pred_tr, average=\"weighted\")\n",
    "    va_macro = f1_score(y_va, pred_va, average=\"macro\")\n",
    "    va_weight = f1_score(y_va, pred_va, average=\"weighted\")\n",
    "\n",
    "    print(f\"Train macro F1:    {tr_macro:.4f}\")\n",
    "    print(f\"Train weighted F1: {tr_weight:.4f}\")\n",
    "    print(f\"Val macro F1:      {va_macro:.4f}\")\n",
    "    print(f\"Val weighted F1:   {va_weight:.4f}\")\n",
    "\n",
    "    tr_macro_list.append(tr_macro)\n",
    "    tr_weight_list.append(tr_weight)\n",
    "    va_macro_list.append(va_macro)\n",
    "    va_weight_list.append(va_weight)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train macro   = {np.mean(tr_macro_list):.4f} ¬± {np.std(tr_macro_list):.4f}\")\n",
    "print(f\"Train weighted= {np.mean(tr_weight_list):.4f} ¬± {np.std(tr_weight_list):.4f}\")\n",
    "print(f\"Val   macro   = {np.mean(va_macro_list):.4f} ¬± {np.std(va_macro_list):.4f}\")\n",
    "print(f\"Val   weighted= {np.mean(va_weight_list):.4f} ¬± {np.std(va_weight_list):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
