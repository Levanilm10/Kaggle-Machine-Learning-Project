{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5292d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587c072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f166e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fa6c",
   "metadata": {},
   "source": [
    "# <u> Data Preprocessing and Cleaning <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['change_type'].map(change_type_map) #we convert the target variable to numeric values using the dictionary we created earlier\n",
    "x_train=train_df.drop(columns=['change_type','index'])#we drop index and change_type columns from the training data\n",
    "x_test=test_df.drop(columns=['index'])#we drop index column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3df0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee766b",
   "metadata": {},
   "source": [
    "### Geometry features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c87ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_features(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    gdf_m = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    area = gdf_m.geometry.area\n",
    "    perim = gdf_m.geometry.length\n",
    "\n",
    "    gdf_ll = gdf.to_crs(epsg=4326)\n",
    "    cent = gdf_ll.geometry.centroid\n",
    "    bounds = gdf_ll.geometry.bounds \n",
    "\n",
    "    gdf[\"geom_area_m2\"] = area\n",
    "    gdf[\"geom_perim_m\"] = perim\n",
    "    gdf[\"geom_compactness\"] = (4 * np.pi * area) / (perim**2 + 1e-9)\n",
    "\n",
    "    gdf[\"centroid_lon\"] = cent.x\n",
    "    gdf[\"centroid_lat\"] = cent.y\n",
    "\n",
    "    gdf[\"bbox_width_deg\"]  = bounds[\"maxx\"] - bounds[\"minx\"]\n",
    "    gdf[\"bbox_height_deg\"] = bounds[\"maxy\"] - bounds[\"miny\"]\n",
    "    gdf[\"bbox_area_deg2\"]  = gdf[\"bbox_width_deg\"] * gdf[\"bbox_height_deg\"]\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc06c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "#for x_train\n",
    "x_train = add_geometry_features(x_train)\n",
    "x_train = x_train.drop(columns=[\"geometry\"])\n",
    "\n",
    "# for x_test\n",
    "x_test = add_geometry_features(x_test)\n",
    "x_test = x_test.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ff2e6",
   "metadata": {},
   "source": [
    "### Date features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6dab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _detect_date_indices(df):\n",
    "    date_cols = [c for c in df.columns if re.fullmatch(r\"date\\d+\", c)]\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"Aucune colonne dateX trouvÃ©e (date0.. ou date1..)\")\n",
    "    idxs = sorted([int(c.replace(\"date\",\"\")) for c in date_cols])\n",
    "    return idxs, [f\"date{i}\" for i in idxs]\n",
    "\n",
    "def add_temporal_status_image_features(df: pd.DataFrame, drop_raw: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Trie chaque ligne par les vraies dates (NaT Ã  la fin)\n",
    "    - Construit des features de statuts (ordre + transitions)\n",
    "    - Construit des features RGB (first/last/deltas) aprÃ¨s tri\n",
    "    - Ajoute aussi date_span_days + deltas entre dates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    idxs, date_cols = _detect_date_indices(df)\n",
    "    k = len(date_cols)\n",
    "\n",
    "    status_cols = [f\"change_status_date{i}\" for i in idxs if f\"change_status_date{i}\" in df.columns]\n",
    "    if len(status_cols) != k:\n",
    "\n",
    "        status_cols = [c for c in df.columns if re.fullmatch(r\"change_status_date\\d+\", c)]\n",
    "        status_idxs = sorted([int(c.split(\"date\")[-1]) for c in status_cols])\n",
    "        status_cols = [f\"change_status_date{i}\" for i in status_idxs]\n",
    "        k = min(len(date_cols), len(status_cols))\n",
    "        date_cols, status_cols, idxs = date_cols[:k], status_cols[:k], idxs[:k]\n",
    "\n",
    "    img_specs = []\n",
    "    for color in [\"red\", \"green\", \"blue\"]:\n",
    "        for stat in [\"mean\", \"std\"]:\n",
    "            cols = []\n",
    "            ok = True\n",
    "            for i in idxs[:k]:\n",
    "                c = f\"img_{color}_{stat}_date{i}\"\n",
    "                if c in df.columns:\n",
    "                    cols.append(c)\n",
    "                else:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                img_specs.append((color, stat, cols))\n",
    "\n",
    "\n",
    "    D = df[date_cols].apply(pd.to_datetime, dayfirst=True, errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    Di = D.astype(\"int64\")\n",
    "    nat_mask = (Di == np.iinfo(np.int64).min)\n",
    "    Di[nat_mask] = np.iinfo(np.int64).max\n",
    "\n",
    "    order = np.argsort(Di, axis=1)\n",
    "    Di_sorted = np.take_along_axis(Di, order, axis=1)\n",
    "\n",
    "    maxv = np.iinfo(np.int64).max\n",
    "    valid_mask = (Di_sorted != maxv)\n",
    "    last_idx = valid_mask.sum(axis=1) - 1\n",
    "\n",
    "\n",
    "    span_days = np.full(len(df), np.nan)\n",
    "    ok = last_idx >= 0\n",
    "    span_days[ok] = (Di_sorted[ok, last_idx[ok]] - Di_sorted[ok, 0]) / (24 * 3600 * 1e9)\n",
    "    df[\"date_span_days\"] = span_days\n",
    "\n",
    "    for j in range(k - 1):\n",
    "        a, b = Di_sorted[:, j], Di_sorted[:, j + 1]\n",
    "        delta = (b - a) / (24 * 3600 * 1e9)\n",
    "        bad = (a == maxv) | (b == maxv)\n",
    "        delta[bad] = np.nan\n",
    "        df[f\"delta_sorted_{j}_{j+1}_days\"] = delta\n",
    "\n",
    "\n",
    "    S = df[status_cols].astype(object).to_numpy()\n",
    "    S_sorted = np.take_along_axis(S, order[:, :k], axis=1)\n",
    "\n",
    "\n",
    "    Sdf = pd.DataFrame(S_sorted)\n",
    "    df[\"status_first\"] = Sdf.bfill(axis=1).iloc[:, 0]\n",
    "    df[\"status_last\"]  = Sdf.ffill(axis=1).iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_first\"], prefix=\"status_first\", dummy_na=True)], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_last\"],  prefix=\"status_last\",  dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    level_map = {\n",
    "        \"Land Cleared\": 0,\n",
    "        \"Construction Started\": 1,\n",
    "        \"Materials Dumped\": 1,\n",
    "        \"Construction Midway\": 2,\n",
    "        \"Construction Done\": 3,\n",
    "        \"Prior Construction\": 3,\n",
    "        \"Operational\": 4,\n",
    "    }\n",
    "\n",
    "    L = np.full((len(df), k), np.nan)\n",
    "    for j in range(k):\n",
    "        L[:, j] = pd.Series(S_sorted[:, j]).map(level_map).to_numpy(dtype=float)\n",
    "        df[f\"status_level_sorted_{j}\"] = L[:, j]\n",
    "\n",
    "    Ldf = pd.DataFrame(L)\n",
    "    first_level = Ldf.bfill(axis=1).iloc[:, 0].to_numpy()\n",
    "    last_level  = Ldf.ffill(axis=1).iloc[:, -1].to_numpy()\n",
    "\n",
    "    df[\"status_level_first\"] = first_level\n",
    "    df[\"status_level_last\"]  = last_level\n",
    "    df[\"status_progress\"]    = last_level - first_level\n",
    "    df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
    "    df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
    "\n",
    "\n",
    "    mask = ~pd.DataFrame(S_sorted).isna().to_numpy()\n",
    "    n_changes = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        m = mask[:, j] & mask[:, j + 1]\n",
    "        n_changes += (m & (S_sorted[:, j] != S_sorted[:, j + 1]))\n",
    "    df[\"status_n_changes\"] = n_changes\n",
    "\n",
    "    n_reg = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        a, b = L[:, j], L[:, j + 1]\n",
    "        m = np.isfinite(a) & np.isfinite(b)\n",
    "        n_reg += (m & (b < a))\n",
    "    df[\"status_n_regressions\"] = n_reg\n",
    "    df[\"status_has_regression\"] = (n_reg > 0).astype(int)\n",
    "\n",
    "\n",
    "    first_date = Di_sorted[:, 0]\n",
    "    time_to_done = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)):\n",
    "        if first_date[i] == maxv:\n",
    "            continue\n",
    "        idx_done = None\n",
    "        for j in range(k):\n",
    "            if valid_mask[i, j] and np.isfinite(L[i, j]) and L[i, j] >= 3:\n",
    "                idx_done = j\n",
    "                break\n",
    "        if idx_done is not None:\n",
    "            time_to_done[i] = (Di_sorted[i, idx_done] - first_date[i]) / (24 * 3600 * 1e9)\n",
    "    df[\"time_to_done_days\"] = time_to_done\n",
    "\n",
    "\n",
    "    for color, stat, cols in img_specs:\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        Xs = np.take_along_axis(X, order[:, :k], axis=1)\n",
    "        Xs = Xs.copy()\n",
    "        Xs[~valid_mask] = np.nan\n",
    "\n",
    "        first = Xs[:, 0]\n",
    "        last = np.full(len(df), np.nan)\n",
    "        ok = last_idx >= 0\n",
    "        last[ok] = Xs[ok, last_idx[ok]]\n",
    "\n",
    "        df[f\"img_{color}_{stat}_first\"] = first\n",
    "        df[f\"img_{color}_{stat}_last\"]  = last\n",
    "        df[f\"img_{color}_{stat}_delta\"] = last - first\n",
    "\n",
    "        for j in range(k - 1):\n",
    "            df[f\"img_{color}_{stat}_delta_{j}_{j+1}\"] = Xs[:, j + 1] - Xs[:, j]\n",
    "\n",
    "\n",
    "    if drop_raw:\n",
    "        drop_cols = []\n",
    "        drop_cols += date_cols + status_cols\n",
    "        for _, _, cols in img_specs:\n",
    "            drop_cols += cols\n",
    "        drop_cols += [\"status_first\", \"status_last\"]\n",
    "        df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622d8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4447/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n"
     ]
    }
   ],
   "source": [
    "#create temporal features for both train and test\n",
    "x_train = add_temporal_status_image_features(x_train)\n",
    "x_test = add_temporal_status_image_features(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4ef1e",
   "metadata": {},
   "source": [
    "### Multi-hot encoding: urban_type & geography_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analyse des valeurs:\n",
      "urban_type - Train: 5, Test: 5, Total: 5\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n",
      "\n",
      "geography_type - Train: 11, Test: 11, Total: 11\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n"
     ]
    }
   ],
   "source": [
    "def multi_hot_encode(df: pd.DataFrame, column: str, all_values: set = None) -> tuple:\n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    if all_values is None:\n",
    "        all_values = set()\n",
    "        for val in df[column].dropna():\n",
    "            if pd.notna(val) and val != 'N,A':  \n",
    "\n",
    "                values = [v.strip() for v in str(val).split(',')]\n",
    "                all_values.update(values)\n",
    "    \n",
    "\n",
    "    for value in sorted(all_values):\n",
    "\n",
    "        col_name = f\"{column}_{value.replace(' ', '_').replace('/', '_')}\"\n",
    "        \n",
    "\n",
    "        df[col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x).split(',') else 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df, all_values\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_values(df: pd.DataFrame, column: str) -> set:\n",
    "    \"\"\"Extrait toutes les valeurs uniques d'une colonne.\"\"\"\n",
    "    all_values = set()\n",
    "    for val in df[column].dropna():\n",
    "        if pd.notna(val) and val != 'N,A':\n",
    "            values = [v.strip() for v in str(val).split(',')]\n",
    "            all_values.update(values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "urban_values_train = extract_all_values(x_train, 'urban_type')\n",
    "urban_values_test = extract_all_values(test_df, 'urban_type')\n",
    "urban_values_all = urban_values_train | urban_values_test  \n",
    "\n",
    "geo_values_train = extract_all_values(x_train, 'geography_type')\n",
    "geo_values_test = extract_all_values(test_df, 'geography_type')\n",
    "geo_values_all = geo_values_train | geo_values_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3301c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296146, 95)\n",
      "Test shape: (120526, 95)\n"
     ]
    }
   ],
   "source": [
    "#multi-hot encoding on train\n",
    "x_train, _ = multi_hot_encode(x_train, 'urban_type', urban_values_all)\n",
    "x_train, _ = multi_hot_encode(x_train, 'geography_type', geo_values_all)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "\n",
    "#multi-hot encoding on test\n",
    "x_test, _ = multi_hot_encode(x_test, 'urban_type', urban_values_all)\n",
    "x_test, _ = multi_hot_encode(x_test, 'geography_type', geo_values_all)\n",
    "print(f\"Test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0e146",
   "metadata": {},
   "source": [
    "### Drop additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"delta_sorted_0_1_days\",\"delta_sorted_1_2_days\", \"delta_sorted_2_3_days\", \"delta_sorted_3_4_days\",\"index\"]\n",
    "\n",
    "\n",
    "cols_to_drop_train = [col for col in cols_to_drop if col in x_train.columns]\n",
    "cols_to_drop_test = [col for col in cols_to_drop if col in x_test.columns]\n",
    "\n",
    "x_train = x_train.drop(columns=cols_to_drop_train)\n",
    "x_test = x_test.drop(columns=cols_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec455e",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_count_train = np.isinf(x_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_count_test = np.isinf(x_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Train: {inf_count_train} infinity values\")\n",
    "print(f\"Test: {inf_count_test} infinity values\")\n",
    "\n",
    "# Replace inf values with NaN for imputation\n",
    "x_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
    "x_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_final = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols_final = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute x_train\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_train[numerical_cols_final] = numerical_imputer.fit_transform(x_train[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_train[categorical_cols_final] = categorical_imputer.fit_transform(x_train[categorical_cols_final])\n",
    "\n",
    "# Impute x_test \n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_test[numerical_cols_final] = numerical_imputer.transform(x_test[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_test[categorical_cols_final] = categorical_imputer.transform(x_test[categorical_cols_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c49bd",
   "metadata": {},
   "source": [
    "# <u> LightGBM Modele training <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b057167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c601a",
   "metadata": {},
   "source": [
    "### Grid-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = [\n",
    "    # initial baseline\n",
    "    dict(),\n",
    "\n",
    "    # more regularization\n",
    "    dict(num_leaves=63, min_child_samples=25, colsample_bytree=0.8, reg_lambda=1.0),\n",
    "\n",
    "    # learnin rate smalller and more leaves for better performances but risk of overfitting\n",
    "    dict(learning_rate=0.03, num_leaves=127, min_child_samples=10),\n",
    "\n",
    "    # regularization L1+L2  \n",
    "    dict(reg_alpha=0.5, reg_lambda=2.0, min_child_samples=10, num_leaves=63),\n",
    "    \n",
    "    # try more_trees\n",
    "    dict(n_estimators=2500, num_leaves=127, min_child_samples=7),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a7048",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.values\n",
    "y = y_train.values\n",
    "\n",
    "classes_global = np.unique(y)\n",
    "num_class = len(classes_global)\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "for i, extra in enumerate(param_list, start=1):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMBO #{i}  params={extra}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    tr_macro_list, tr_weight_list = [], []\n",
    "    va_macro_list, va_weight_list = [], []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y), start=1):\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # class_weight \n",
    "        w = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_tr), y=y_tr)\n",
    "        class_weight = {c: ww for c, ww in zip(np.unique(y_tr), w)}\n",
    "\n",
    "        # base params\n",
    "        base_params = {\n",
    "            'objective': \"multiclass\",\n",
    "            'num_class': num_class,\n",
    "            'n_estimators': 1500,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 63,\n",
    "            'min_child_samples': 15,\n",
    "            'subsample': 0.9,\n",
    "            'subsample_freq': 1,\n",
    "            'colsample_bytree': 0.9,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 0.0,\n",
    "            'class_weight': class_weight,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        # fusion with extra params for this combo\n",
    "        base_params.update(extra)\n",
    "        \n",
    "        model = LGBMClassifier(**base_params)\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"multi_logloss\",\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "        )\n",
    "\n",
    "        pred_tr = model.predict(X_tr)\n",
    "        pred_va = model.predict(X_va)\n",
    "\n",
    "        tr_macro = f1_score(y_tr, pred_tr, average=\"macro\")\n",
    "        tr_weight = f1_score(y_tr, pred_tr, average=\"weighted\")\n",
    "        va_macro = f1_score(y_va, pred_va, average=\"macro\")\n",
    "        va_weight = f1_score(y_va, pred_va, average=\"weighted\")\n",
    "\n",
    "        tr_macro_list.append(tr_macro); tr_weight_list.append(tr_weight)\n",
    "        va_macro_list.append(va_macro); va_weight_list.append(va_weight)\n",
    "\n",
    "\n",
    "    print(\"\\n--- MEAN Â± STD over 3 folds ---\")\n",
    "    print(f\"Train macro   = {np.mean(tr_macro_list):.4f} Â± {np.std(tr_macro_list):.4f}\")\n",
    "    print(f\"Train weighted= {np.mean(tr_weight_list):.4f} Â± {np.std(tr_weight_list):.4f}\")\n",
    "    print(f\"Val   macro   = {np.mean(va_macro_list):.4f} Â± {np.std(va_macro_list):.4f}\")\n",
    "    print(f\"Val   weighted= {np.mean(va_weight_list):.4f} Â± {np.std(va_weight_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f5fe4",
   "metadata": {},
   "source": [
    "### RÃ©sultats:\n",
    "\n",
    "\n",
    "|  | HyperparamÃ¨tres | Train Macro-F1 | Train Weighted-F1 | Val Macro-F1 | Val Weighted-F1 |\n",
    "|------:|----------------------------|----------------|-------------------|--------------|-----------------|\n",
    "| 1 | `dict()` | 0.9372 Â± 0.0009 | 0.8856 Â± 0.0016 | 0.5451 Â± 0.0061 | 0.7722 Â± 0.0008 |\n",
    "| 2 | `dict(num_leaves=63, min_child_samples=25, colsample_bytree=0.8, reg_lambda=1.0)` | 0.9367 Â± 0.0009 | 0.8869 Â± 0.0017 | 0.5509 Â± 0.0061 | 0.7738 Â± 0.0006 |\n",
    "| 3 | `dict(learning_rate=0.03, num_leaves=127, min_child_samples=10)` | 0.9388 Â± 0.0006 | 0.8873 Â± 0.0006 | 0.5476 Â± 0.0055 | 0.7727 Â± 0.0002 |\n",
    "| 4 | `dict(reg_alpha=0.5, reg_lambda=2.0, min_child_samples=10, num_leaves=63)` | 0.9405 Â± 0.0010 | 0.8935 Â± 0.0017 | 0.5523 Â± 0.0055 | 0.7744 Â± 0.0006 |\n",
    "| 5 | `dict(n_estimators=2500, num_leaves=127, min_child_samples=7)` | 0.9999 Â± 0.0000 | 0.9998 Â± 0.0000 | 0.5461 Â± 0.0044 | 0.7792 Â± 0.0016 |\n",
    "\n",
    "\n",
    "Based solely on the validation weighted F1-score, the best performance is achieved by Combo 5 with 0.7792 Â± 0.0016. Beyond having the highest mean score, its low standard deviation (still two times bigger than the other configuratiosn) indicates that the performance is stable across the three splits, which further supports selecting this configuration. However, this model reaches an almost perfect training weighted F1-score (â‰ˆ 1.00), indicating a risk of overfitting. In contrast, Combo 4 (L1+L2 regularization) provides a more conservative trade-off with 0.7744 Â± 0.0006 on validation while remaining competitive.\n",
    "Selected choice: to maximize the weighted F1 while maintaining stable CV performance, Combo 5 is selected. For a more cautious and better-regularized variant, Combo 4 is the main alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f60666",
   "metadata": {},
   "source": [
    "# <u> Train on the whole training dataset <u/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f071772",
   "metadata": {},
   "source": [
    "### Class balancing: compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c35a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight = {c: w for c, w in zip(classes, weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78c430",
   "metadata": {},
   "source": [
    "### Train lgbm classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=6,\n",
    "    n_estimators=2500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=127, \n",
    "    min_child_samples=7,\n",
    "    subsample=0.9,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=1,\n",
    "    class_weight=class_weight,\n",
    ")\n",
    "\n",
    "lgbm_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc3431",
   "metadata": {},
   "source": [
    "# <u>Create the submission file<u/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbf9b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lgbm_model.predict(x_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_df.index,\n",
    "    'change_type': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_file = 'lgb_submission_over.csv'\n",
    "submission.to_csv(submission_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
