{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5292d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587c072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f166e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fa6c",
   "metadata": {},
   "source": [
    "# <u> Data Preprocessing and Cleaning <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['change_type'].map(change_type_map) #we convert the target variable to numeric values using the dictionary we created earlier\n",
    "x_train=train_df.drop(columns=['change_type','index'])#we drop index and change_type columns from the training data\n",
    "x_test=test_df.drop(columns=['index'])#we drop index column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3df0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee766b",
   "metadata": {},
   "source": [
    "### Geometry features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c87ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_features(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    gdf_m = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    area = gdf_m.geometry.area\n",
    "    perim = gdf_m.geometry.length\n",
    "\n",
    "    gdf_ll = gdf.to_crs(epsg=4326)\n",
    "    cent = gdf_ll.geometry.centroid\n",
    "    bounds = gdf_ll.geometry.bounds \n",
    "\n",
    "    gdf[\"geom_area_m2\"] = area\n",
    "    gdf[\"geom_perim_m\"] = perim\n",
    "    gdf[\"geom_compactness\"] = (4 * np.pi * area) / (perim**2 + 1e-9)\n",
    "\n",
    "    gdf[\"centroid_lon\"] = cent.x\n",
    "    gdf[\"centroid_lat\"] = cent.y\n",
    "\n",
    "    gdf[\"bbox_width_deg\"]  = bounds[\"maxx\"] - bounds[\"minx\"]\n",
    "    gdf[\"bbox_height_deg\"] = bounds[\"maxy\"] - bounds[\"miny\"]\n",
    "    gdf[\"bbox_area_deg2\"]  = gdf[\"bbox_width_deg\"] * gdf[\"bbox_height_deg\"]\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc06c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "#for x_train\n",
    "x_train = add_geometry_features(x_train)\n",
    "x_train = x_train.drop(columns=[\"geometry\"])\n",
    "\n",
    "# for x_test\n",
    "x_test = add_geometry_features(x_test)\n",
    "x_test = x_test.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ff2e6",
   "metadata": {},
   "source": [
    "### Date features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6dab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _detect_date_indices(df):\n",
    "    date_cols = [c for c in df.columns if re.fullmatch(r\"date\\d+\", c)]\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"Aucune colonne dateX trouv√©e (date0.. ou date1..)\")\n",
    "    idxs = sorted([int(c.replace(\"date\",\"\")) for c in date_cols])\n",
    "    return idxs, [f\"date{i}\" for i in idxs]\n",
    "\n",
    "def add_temporal_status_image_features(df: pd.DataFrame, drop_raw: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Trie chaque ligne par les vraies dates (NaT √† la fin)\n",
    "    - Construit des features de statuts (ordre + transitions)\n",
    "    - Construit des features RGB (first/last/deltas) apr√®s tri\n",
    "    - Ajoute aussi date_span_days + deltas entre dates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    idxs, date_cols = _detect_date_indices(df)\n",
    "    k = len(date_cols)\n",
    "\n",
    "    status_cols = [f\"change_status_date{i}\" for i in idxs if f\"change_status_date{i}\" in df.columns]\n",
    "    if len(status_cols) != k:\n",
    "\n",
    "        status_cols = [c for c in df.columns if re.fullmatch(r\"change_status_date\\d+\", c)]\n",
    "        status_idxs = sorted([int(c.split(\"date\")[-1]) for c in status_cols])\n",
    "        status_cols = [f\"change_status_date{i}\" for i in status_idxs]\n",
    "        k = min(len(date_cols), len(status_cols))\n",
    "        date_cols, status_cols, idxs = date_cols[:k], status_cols[:k], idxs[:k]\n",
    "\n",
    "    img_specs = []\n",
    "    for color in [\"red\", \"green\", \"blue\"]:\n",
    "        for stat in [\"mean\", \"std\"]:\n",
    "            cols = []\n",
    "            ok = True\n",
    "            for i in idxs[:k]:\n",
    "                c = f\"img_{color}_{stat}_date{i}\"\n",
    "                if c in df.columns:\n",
    "                    cols.append(c)\n",
    "                else:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                img_specs.append((color, stat, cols))\n",
    "\n",
    "\n",
    "    D = df[date_cols].apply(pd.to_datetime, dayfirst=True, errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    Di = D.astype(\"int64\")\n",
    "    nat_mask = (Di == np.iinfo(np.int64).min)\n",
    "    Di[nat_mask] = np.iinfo(np.int64).max\n",
    "\n",
    "    order = np.argsort(Di, axis=1)\n",
    "    Di_sorted = np.take_along_axis(Di, order, axis=1)\n",
    "\n",
    "    maxv = np.iinfo(np.int64).max\n",
    "    valid_mask = (Di_sorted != maxv)\n",
    "    last_idx = valid_mask.sum(axis=1) - 1\n",
    "\n",
    "\n",
    "    span_days = np.full(len(df), np.nan)\n",
    "    ok = last_idx >= 0\n",
    "    span_days[ok] = (Di_sorted[ok, last_idx[ok]] - Di_sorted[ok, 0]) / (24 * 3600 * 1e9)\n",
    "    df[\"date_span_days\"] = span_days\n",
    "\n",
    "    for j in range(k - 1):\n",
    "        a, b = Di_sorted[:, j], Di_sorted[:, j + 1]\n",
    "        delta = (b - a) / (24 * 3600 * 1e9)\n",
    "        bad = (a == maxv) | (b == maxv)\n",
    "        delta[bad] = np.nan\n",
    "        df[f\"delta_sorted_{j}_{j+1}_days\"] = delta\n",
    "\n",
    "\n",
    "    S = df[status_cols].astype(object).to_numpy()\n",
    "    S_sorted = np.take_along_axis(S, order[:, :k], axis=1)\n",
    "\n",
    "\n",
    "    Sdf = pd.DataFrame(S_sorted)\n",
    "    df[\"status_first\"] = Sdf.bfill(axis=1).iloc[:, 0]\n",
    "    df[\"status_last\"]  = Sdf.ffill(axis=1).iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_first\"], prefix=\"status_first\", dummy_na=True)], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_last\"],  prefix=\"status_last\",  dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    level_map = {\n",
    "        \"Land Cleared\": 0,\n",
    "        \"Construction Started\": 1,\n",
    "        \"Materials Dumped\": 1,\n",
    "        \"Construction Midway\": 2,\n",
    "        \"Construction Done\": 3,\n",
    "        \"Prior Construction\": 3,\n",
    "        \"Operational\": 4,\n",
    "    }\n",
    "\n",
    "    L = np.full((len(df), k), np.nan)\n",
    "    for j in range(k):\n",
    "        L[:, j] = pd.Series(S_sorted[:, j]).map(level_map).to_numpy(dtype=float)\n",
    "        df[f\"status_level_sorted_{j}\"] = L[:, j]\n",
    "\n",
    "    Ldf = pd.DataFrame(L)\n",
    "    first_level = Ldf.bfill(axis=1).iloc[:, 0].to_numpy()\n",
    "    last_level  = Ldf.ffill(axis=1).iloc[:, -1].to_numpy()\n",
    "\n",
    "    df[\"status_level_first\"] = first_level\n",
    "    df[\"status_level_last\"]  = last_level\n",
    "    df[\"status_progress\"]    = last_level - first_level\n",
    "    df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
    "    df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
    "\n",
    "\n",
    "    mask = ~pd.DataFrame(S_sorted).isna().to_numpy()\n",
    "    n_changes = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        m = mask[:, j] & mask[:, j + 1]\n",
    "        n_changes += (m & (S_sorted[:, j] != S_sorted[:, j + 1]))\n",
    "    df[\"status_n_changes\"] = n_changes\n",
    "\n",
    "    n_reg = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        a, b = L[:, j], L[:, j + 1]\n",
    "        m = np.isfinite(a) & np.isfinite(b)\n",
    "        n_reg += (m & (b < a))\n",
    "    df[\"status_n_regressions\"] = n_reg\n",
    "    df[\"status_has_regression\"] = (n_reg > 0).astype(int)\n",
    "\n",
    "\n",
    "    first_date = Di_sorted[:, 0]\n",
    "    time_to_done = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)):\n",
    "        if first_date[i] == maxv:\n",
    "            continue\n",
    "        idx_done = None\n",
    "        for j in range(k):\n",
    "            if valid_mask[i, j] and np.isfinite(L[i, j]) and L[i, j] >= 3:\n",
    "                idx_done = j\n",
    "                break\n",
    "        if idx_done is not None:\n",
    "            time_to_done[i] = (Di_sorted[i, idx_done] - first_date[i]) / (24 * 3600 * 1e9)\n",
    "    df[\"time_to_done_days\"] = time_to_done\n",
    "\n",
    "\n",
    "    for color, stat, cols in img_specs:\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        Xs = np.take_along_axis(X, order[:, :k], axis=1)\n",
    "        Xs = Xs.copy()\n",
    "        Xs[~valid_mask] = np.nan\n",
    "\n",
    "        first = Xs[:, 0]\n",
    "        last = np.full(len(df), np.nan)\n",
    "        ok = last_idx >= 0\n",
    "        last[ok] = Xs[ok, last_idx[ok]]\n",
    "\n",
    "        df[f\"img_{color}_{stat}_first\"] = first\n",
    "        df[f\"img_{color}_{stat}_last\"]  = last\n",
    "        df[f\"img_{color}_{stat}_delta\"] = last - first\n",
    "\n",
    "        for j in range(k - 1):\n",
    "            df[f\"img_{color}_{stat}_delta_{j}_{j+1}\"] = Xs[:, j + 1] - Xs[:, j]\n",
    "\n",
    "\n",
    "    if drop_raw:\n",
    "        drop_cols = []\n",
    "        drop_cols += date_cols + status_cols\n",
    "        for _, _, cols in img_specs:\n",
    "            drop_cols += cols\n",
    "        drop_cols += [\"status_first\", \"status_last\"]\n",
    "        df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622d8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_4690/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n"
     ]
    }
   ],
   "source": [
    "#create temporal features for both train and test\n",
    "x_train = add_temporal_status_image_features(x_train)\n",
    "x_test = add_temporal_status_image_features(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4ef1e",
   "metadata": {},
   "source": [
    "### Multi-hot encoding: urban_type & geography_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyse des valeurs:\n",
      "urban_type - Train: 5, Test: 5, Total: 5\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n",
      "\n",
      "geography_type - Train: 11, Test: 11, Total: 11\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n"
     ]
    }
   ],
   "source": [
    "def multi_hot_encode(df: pd.DataFrame, column: str, all_values: set = None) -> tuple:\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    if all_values is None:\n",
    "        all_values = set()\n",
    "        for val in df[column].dropna():\n",
    "            if pd.notna(val) and val != 'N,A':  \n",
    "\n",
    "                values = [v.strip() for v in str(val).split(',')]\n",
    "                all_values.update(values)\n",
    "    \n",
    "\n",
    "    for value in sorted(all_values):\n",
    "\n",
    "        col_name = f\"{column}_{value.replace(' ', '_').replace('/', '_')}\"\n",
    "        \n",
    "\n",
    "        df[col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x).split(',') else 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df, all_values\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_values(df: pd.DataFrame, column: str) -> set:\n",
    "    \"\"\"Extrait toutes les valeurs uniques d'une colonne.\"\"\"\n",
    "    all_values = set()\n",
    "    for val in df[column].dropna():\n",
    "        if pd.notna(val) and val != 'N,A':\n",
    "            values = [v.strip() for v in str(val).split(',')]\n",
    "            all_values.update(values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "urban_values_train = extract_all_values(x_train, 'urban_type')\n",
    "urban_values_test = extract_all_values(test_df, 'urban_type')\n",
    "urban_values_all = urban_values_train | urban_values_test  \n",
    "\n",
    "geo_values_train = extract_all_values(x_train, 'geography_type')\n",
    "geo_values_test = extract_all_values(test_df, 'geography_type')\n",
    "geo_values_all = geo_values_train | geo_values_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3301c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296146, 95)\n",
      "Test shape: (120526, 95)\n"
     ]
    }
   ],
   "source": [
    "#multi-hot encoding on train\n",
    "x_train, _ = multi_hot_encode(x_train, 'urban_type', urban_values_all)\n",
    "x_train, _ = multi_hot_encode(x_train, 'geography_type', geo_values_all)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "\n",
    "#multi-hot encoding on test\n",
    "x_test, _ = multi_hot_encode(x_test, 'urban_type', urban_values_all)\n",
    "x_test, _ = multi_hot_encode(x_test, 'geography_type', geo_values_all)\n",
    "print(f\"Test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0e146",
   "metadata": {},
   "source": [
    "### Drop additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8c664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"delta_sorted_0_1_days\",\"delta_sorted_1_2_days\", \"delta_sorted_2_3_days\", \"delta_sorted_3_4_days\",\"index\"]\n",
    "\n",
    "\n",
    "cols_to_drop_train = [col for col in cols_to_drop if col in x_train.columns]\n",
    "cols_to_drop_test = [col for col in cols_to_drop if col in x_test.columns]\n",
    "\n",
    "x_train = x_train.drop(columns=cols_to_drop_train)\n",
    "x_test = x_test.drop(columns=cols_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec455e",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f128af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 infinity values\n",
      "Test: 2 infinity values\n"
     ]
    }
   ],
   "source": [
    "inf_count_train = np.isinf(x_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_count_test = np.isinf(x_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Train: {inf_count_train} infinity values\")\n",
    "print(f\"Test: {inf_count_test} infinity values\")\n",
    "\n",
    "# Replace inf values with NaN for imputation\n",
    "x_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
    "x_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_final = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols_final = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute x_train\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_train[numerical_cols_final] = numerical_imputer.fit_transform(x_train[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_train[categorical_cols_final] = categorical_imputer.fit_transform(x_train[categorical_cols_final])\n",
    "\n",
    "# Impute x_test \n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_test[numerical_cols_final] = numerical_imputer.transform(x_test[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_test[categorical_cols_final] = categorical_imputer.transform(x_test[categorical_cols_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c49bd",
   "metadata": {},
   "source": [
    "# <u> KNN Modele training <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0235178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN  macro F1   : 1.0\n",
      "TRAIN  weighted F1: 1.0\n",
      "VALID  macro F1   : 0.45330405585798017\n",
      "VALID  weighted F1: 0.7019477379899111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = x_train.values\n",
    "y = y_train.values\n",
    "\n",
    "# Split stratifi√© 80/20\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, va_idx = next(sss.split(X, y))\n",
    "X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "knn = make_pipeline(\n",
    "    StandardScaler(),  # indispensable\n",
    "    KNeighborsClassifier(\n",
    "        n_neighbors=35,\n",
    "        weights=\"distance\",\n",
    "        metric=\"minkowski\",\n",
    "        p=2,\n",
    "        algorithm=\"brute\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "knn.fit(X_tr, y_tr)\n",
    "\n",
    "pred_tr = knn.predict(X_tr)\n",
    "pred_va = knn.predict(X_va)\n",
    "\n",
    "print(\"TRAIN  macro F1   :\", f1_score(y_tr, pred_tr, average=\"macro\"))\n",
    "print(\"TRAIN  weighted F1:\", f1_score(y_tr, pred_tr, average=\"weighted\"))\n",
    "print(\"VALID  macro F1   :\", f1_score(y_va, pred_va, average=\"macro\"))\n",
    "print(\"VALID  weighted F1:\", f1_score(y_va, pred_va, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601316e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Val weighted F1: 0.7117747284349748 | Val macro F1: 0.468454508585159\n",
      "75 Val weighted F1: 0.690243952460002 | Val macro F1: 0.43842933671719275\n"
     ]
    }
   ],
   "source": [
    "for k in [15, 75]:\n",
    "    knn = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        KNeighborsClassifier(n_neighbors=k, weights=\"distance\",\n",
    "                             metric=\"minkowski\", p=2,\n",
    "                             algorithm=\"brute\", n_jobs=-1)\n",
    "    )\n",
    "    knn.fit(X_tr, y_tr)\n",
    "    pred_va = knn.predict(X_va)\n",
    "    print(k, \"Val weighted F1:\", f1_score(y_va, pred_va, average=\"weighted\"),\n",
    "              \"| Val macro F1:\", f1_score(y_va, pred_va, average=\"macro\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
