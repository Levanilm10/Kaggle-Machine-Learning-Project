{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c580a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "train_df = gpd.read_file('train.geojson')\n",
    "test_df = gpd.read_file('test.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a50815",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafbdf2",
   "metadata": {},
   "source": [
    "# <u> Data Preprocessing and Cleaning <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311820fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['change_type'].map(change_type_map) #we convert the target variable to numeric values using the dictionary we created earlier\n",
    "x_train=train_df.drop(columns=['change_type','index'])#we drop index and change_type columns from the training data\n",
    "x_test=test_df.drop(columns=['index'])#we drop index column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fdb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec7ae4",
   "metadata": {},
   "source": [
    "### Geometry features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a77d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_features(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "\n",
    "    gdf_m = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    area = gdf_m.geometry.area\n",
    "    perim = gdf_m.geometry.length\n",
    "\n",
    "\n",
    "    gdf_ll = gdf.to_crs(epsg=4326)\n",
    "    cent = gdf_ll.geometry.centroid\n",
    "    bounds = gdf_ll.geometry.bounds  \n",
    "\n",
    "    gdf[\"geom_area_m2\"] = area\n",
    "    gdf[\"geom_perim_m\"] = perim\n",
    "    gdf[\"geom_compactness\"] = (4 * np.pi * area) / (perim**2 + 1e-9)\n",
    "\n",
    "    gdf[\"centroid_lon\"] = cent.x\n",
    "    gdf[\"centroid_lat\"] = cent.y\n",
    "\n",
    "    gdf[\"bbox_width_deg\"]  = bounds[\"maxx\"] - bounds[\"minx\"]\n",
    "    gdf[\"bbox_height_deg\"] = bounds[\"maxy\"] - bounds[\"miny\"]\n",
    "    gdf[\"bbox_area_deg2\"]  = gdf[\"bbox_width_deg\"] * gdf[\"bbox_height_deg\"]\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00827e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3440110158.py:16: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:44: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Users/levani/Library/Python/3.12/lib/python/site-packages/shapely/measurement.py:182: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3440110158.py:16: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for x_train\n",
    "x_train = add_geometry_features(x_train)\n",
    "x_train = x_train.drop(columns=[\"geometry\"])\n",
    "\n",
    "# for x_test\n",
    "x_test = add_geometry_features(x_test)\n",
    "x_test = x_test.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e5e0e",
   "metadata": {},
   "source": [
    "### Date features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e8298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def _detect_date_indices(df):\n",
    "    date_cols = [c for c in df.columns if re.fullmatch(r\"date\\d+\", c)]\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"Aucune colonne dateX trouv√©e (date0.. ou date1..)\")\n",
    "    idxs = sorted([int(c.replace(\"date\",\"\")) for c in date_cols])\n",
    "    return idxs, [f\"date{i}\" for i in idxs]\n",
    "\n",
    "def add_temporal_status_image_features(df: pd.DataFrame, drop_raw: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Trie chaque ligne par les vraies dates (NaT √† la fin)\n",
    "    - Construit des features de statuts (ordre + transitions)\n",
    "    - Construit des features RGB (first/last/deltas) apr√®s tri\n",
    "    - Ajoute aussi date_span_days + deltas entre dates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    idxs, date_cols = _detect_date_indices(df)\n",
    "    k = len(date_cols)\n",
    "\n",
    "    status_cols = [f\"change_status_date{i}\" for i in idxs if f\"change_status_date{i}\" in df.columns]\n",
    "    if len(status_cols) != k:\n",
    "\n",
    "        status_cols = [c for c in df.columns if re.fullmatch(r\"change_status_date\\d+\", c)]\n",
    "        status_idxs = sorted([int(c.split(\"date\")[-1]) for c in status_cols])\n",
    "        status_cols = [f\"change_status_date{i}\" for i in status_idxs]\n",
    "        k = min(len(date_cols), len(status_cols))\n",
    "        date_cols, status_cols, idxs = date_cols[:k], status_cols[:k], idxs[:k]\n",
    "\n",
    "\n",
    "    img_specs = []\n",
    "    for color in [\"red\", \"green\", \"blue\"]:\n",
    "        for stat in [\"mean\", \"std\"]:\n",
    "            cols = []\n",
    "            ok = True\n",
    "            for i in idxs[:k]:\n",
    "                c = f\"img_{color}_{stat}_date{i}\"\n",
    "                if c in df.columns:\n",
    "                    cols.append(c)\n",
    "                else:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                img_specs.append((color, stat, cols))\n",
    "\n",
    "\n",
    "    D = df[date_cols].apply(pd.to_datetime, dayfirst=True, errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    Di = D.astype(\"int64\")\n",
    "    nat_mask = (Di == np.iinfo(np.int64).min)\n",
    "    Di[nat_mask] = np.iinfo(np.int64).max\n",
    "\n",
    "    order = np.argsort(Di, axis=1)\n",
    "    Di_sorted = np.take_along_axis(Di, order, axis=1)\n",
    "\n",
    "    maxv = np.iinfo(np.int64).max\n",
    "    valid_mask = (Di_sorted != maxv)\n",
    "    last_idx = valid_mask.sum(axis=1) - 1\n",
    "\n",
    "\n",
    "    span_days = np.full(len(df), np.nan)\n",
    "    ok = last_idx >= 0\n",
    "    span_days[ok] = (Di_sorted[ok, last_idx[ok]] - Di_sorted[ok, 0]) / (24 * 3600 * 1e9)\n",
    "    df[\"date_span_days\"] = span_days\n",
    "\n",
    "    for j in range(k - 1):\n",
    "        a, b = Di_sorted[:, j], Di_sorted[:, j + 1]\n",
    "        delta = (b - a) / (24 * 3600 * 1e9)\n",
    "        bad = (a == maxv) | (b == maxv)\n",
    "        delta[bad] = np.nan\n",
    "        df[f\"delta_sorted_{j}_{j+1}_days\"] = delta\n",
    "\n",
    "\n",
    "    S = df[status_cols].astype(object).to_numpy()\n",
    "    S_sorted = np.take_along_axis(S, order[:, :k], axis=1)\n",
    "\n",
    "\n",
    "    Sdf = pd.DataFrame(S_sorted)\n",
    "    df[\"status_first\"] = Sdf.bfill(axis=1).iloc[:, 0]\n",
    "    df[\"status_last\"]  = Sdf.ffill(axis=1).iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_first\"], prefix=\"status_first\", dummy_na=True)], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_last\"],  prefix=\"status_last\",  dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    level_map = {\n",
    "        \"Land Cleared\": 0,\n",
    "        \"Construction Started\": 1,\n",
    "        \"Materials Dumped\": 1,\n",
    "        \"Construction Midway\": 2,\n",
    "        \"Construction Done\": 3,\n",
    "        \"Prior Construction\": 3,\n",
    "        \"Operational\": 4,\n",
    "    }\n",
    "\n",
    "    L = np.full((len(df), k), np.nan)\n",
    "    for j in range(k):\n",
    "        L[:, j] = pd.Series(S_sorted[:, j]).map(level_map).to_numpy(dtype=float)\n",
    "        df[f\"status_level_sorted_{j}\"] = L[:, j]\n",
    "\n",
    "    Ldf = pd.DataFrame(L)\n",
    "    first_level = Ldf.bfill(axis=1).iloc[:, 0].to_numpy()\n",
    "    last_level  = Ldf.ffill(axis=1).iloc[:, -1].to_numpy()\n",
    "\n",
    "    df[\"status_level_first\"] = first_level\n",
    "    df[\"status_level_last\"]  = last_level\n",
    "    df[\"status_progress\"]    = last_level - first_level\n",
    "    df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
    "    df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
    "\n",
    "\n",
    "    mask = ~pd.DataFrame(S_sorted).isna().to_numpy()\n",
    "    n_changes = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        m = mask[:, j] & mask[:, j + 1]\n",
    "        n_changes += (m & (S_sorted[:, j] != S_sorted[:, j + 1]))\n",
    "    df[\"status_n_changes\"] = n_changes\n",
    "\n",
    "    n_reg = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        a, b = L[:, j], L[:, j + 1]\n",
    "        m = np.isfinite(a) & np.isfinite(b)\n",
    "        n_reg += (m & (b < a))\n",
    "    df[\"status_n_regressions\"] = n_reg\n",
    "    df[\"status_has_regression\"] = (n_reg > 0).astype(int)\n",
    "\n",
    "\n",
    "    first_date = Di_sorted[:, 0]\n",
    "    time_to_done = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)):\n",
    "        if first_date[i] == maxv:\n",
    "            continue\n",
    "        idx_done = None\n",
    "        for j in range(k):\n",
    "            if valid_mask[i, j] and np.isfinite(L[i, j]) and L[i, j] >= 3:\n",
    "                idx_done = j\n",
    "                break\n",
    "        if idx_done is not None:\n",
    "            time_to_done[i] = (Di_sorted[i, idx_done] - first_date[i]) / (24 * 3600 * 1e9)\n",
    "    df[\"time_to_done_days\"] = time_to_done\n",
    "\n",
    "\n",
    "    for color, stat, cols in img_specs:\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        Xs = np.take_along_axis(X, order[:, :k], axis=1)\n",
    "        Xs = Xs.copy()\n",
    "        Xs[~valid_mask] = np.nan\n",
    "\n",
    "        first = Xs[:, 0]\n",
    "        last = np.full(len(df), np.nan)\n",
    "        ok = last_idx >= 0\n",
    "        last[ok] = Xs[ok, last_idx[ok]]\n",
    "\n",
    "        df[f\"img_{color}_{stat}_first\"] = first\n",
    "        df[f\"img_{color}_{stat}_last\"]  = last\n",
    "        df[f\"img_{color}_{stat}_delta\"] = last - first\n",
    "\n",
    "        for j in range(k - 1):\n",
    "            df[f\"img_{color}_{stat}_delta_{j}_{j+1}\"] = Xs[:, j + 1] - Xs[:, j]\n",
    "\n",
    "\n",
    "    if drop_raw:\n",
    "        drop_cols = []\n",
    "        drop_cols += date_cols + status_cols\n",
    "        for _, _, cols in img_specs:\n",
    "            drop_cols += cols\n",
    "        drop_cols += [\"status_first\", \"status_last\"]\n",
    "        df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e4574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3125709378.py:109: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3125709378.py:110: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3125709378.py:109: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/4y/h39663j137n2w_fwpdn_tw8w0000gp/T/ipykernel_3623/3125709378.py:110: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n"
     ]
    }
   ],
   "source": [
    "#create temporal features for both train and test\n",
    "x_train = add_temporal_status_image_features(x_train)\n",
    "x_test = add_temporal_status_image_features(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ce3a4",
   "metadata": {},
   "source": [
    "### Multi-hot encoding: urban_type & geography_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfcc7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyse des valeurs:\n",
      "urban_type - Train: 5, Test: 5, Total: 5\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n",
      "\n",
      "geography_type - Train: 11, Test: 11, Total: 11\n",
      "  Uniquement dans train: set()\n",
      "  Uniquement dans test: set()\n"
     ]
    }
   ],
   "source": [
    "def multi_hot_encode(df: pd.DataFrame, column: str, all_values: set = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Cr√©e un encodage multi-hot pour une colonne avec des valeurs s√©par√©es par des virgules.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec la colonne √† encoder\n",
    "        column: Nom de la colonne √† encoder\n",
    "        all_values: Ensemble des valeurs possibles (optionnel). Si None, extrait du df.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame encod√©, set des valeurs uniques)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    if all_values is None:\n",
    "        all_values = set()\n",
    "        for val in df[column].dropna():\n",
    "            if pd.notna(val) and val != 'N,A':  \n",
    "\n",
    "                values = [v.strip() for v in str(val).split(',')]\n",
    "                all_values.update(values)\n",
    "    \n",
    "\n",
    "    for value in sorted(all_values):\n",
    "\n",
    "        col_name = f\"{column}_{value.replace(' ', '_').replace('/', '_')}\"\n",
    "        \n",
    "\n",
    "        df[col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x).split(',') else 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df, all_values\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_values(df: pd.DataFrame, column: str) -> set:\n",
    "    \"\"\"Extrait toutes les valeurs uniques d'une colonne.\"\"\"\n",
    "    all_values = set()\n",
    "    for val in df[column].dropna():\n",
    "        if pd.notna(val) and val != 'N,A':\n",
    "            values = [v.strip() for v in str(val).split(',')]\n",
    "            all_values.update(values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "urban_values_train = extract_all_values(x_train, 'urban_type')\n",
    "urban_values_test = extract_all_values(test_df, 'urban_type')\n",
    "urban_values_all = urban_values_train | urban_values_test  \n",
    "\n",
    "geo_values_train = extract_all_values(x_train, 'geography_type')\n",
    "geo_values_test = extract_all_values(test_df, 'geography_type')\n",
    "geo_values_all = geo_values_train | geo_values_test\n",
    "\n",
    "print(\"üîç Analyse des valeurs:\")\n",
    "print(f\"urban_type - Train: {len(urban_values_train)}, Test: {len(urban_values_test)}, Total: {len(urban_values_all)}\")\n",
    "print(f\"  Uniquement dans train: {urban_values_train - urban_values_test}\")\n",
    "print(f\"  Uniquement dans test: {urban_values_test - urban_values_train}\")\n",
    "\n",
    "print(f\"\\ngeography_type - Train: {len(geo_values_train)}, Test: {len(geo_values_test)}, Total: {len(geo_values_all)}\")\n",
    "print(f\"  Uniquement dans train: {geo_values_train - geo_values_train.intersection(geo_values_test) if len(geo_values_train - geo_values_test) <= 5 else '(trop pour afficher)'}\")\n",
    "print(f\"  Uniquement dans test: {geo_values_test - geo_values_test.intersection(geo_values_train) if len(geo_values_test - geo_values_train) <= 5 else '(trop pour afficher)'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b200f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296146, 95)\n",
      "Test shape: (120526, 95)\n"
     ]
    }
   ],
   "source": [
    "#multi-hot encoding on train\n",
    "x_train, _ = multi_hot_encode(x_train, 'urban_type', urban_values_all)\n",
    "x_train, _ = multi_hot_encode(x_train, 'geography_type', geo_values_all)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "\n",
    "#multi-hot encoding on test\n",
    "x_test, _ = multi_hot_encode(x_test, 'urban_type', urban_values_all)\n",
    "x_test, _ = multi_hot_encode(x_test, 'geography_type', geo_values_all)\n",
    "print(f\"Test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c0f67",
   "metadata": {},
   "source": [
    "### Drop additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912d6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"delta_sorted_0_1_days\",\"delta_sorted_1_2_days\", \"delta_sorted_2_3_days\", \"delta_sorted_3_4_days\",\"index\"]\n",
    "\n",
    "\n",
    "cols_to_drop_train = [col for col in cols_to_drop if col in x_train.columns]\n",
    "cols_to_drop_test = [col for col in cols_to_drop if col in x_test.columns]\n",
    "\n",
    "x_train = x_train.drop(columns=cols_to_drop_train)\n",
    "x_test = x_test.drop(columns=cols_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15637a",
   "metadata": {},
   "source": [
    "### Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4410900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 infinity values\n",
      "Test: 2 infinity values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inf_count_train = np.isinf(x_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_count_test = np.isinf(x_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Train: {inf_count_train} infinity values\")\n",
    "print(f\"Test: {inf_count_test} infinity values\")\n",
    "\n",
    "\n",
    "x_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
    "x_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_final = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols_final = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_train[numerical_cols_final] = numerical_imputer.fit_transform(x_train[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_train[categorical_cols_final] = categorical_imputer.fit_transform(x_train[categorical_cols_final])\n",
    "\n",
    "\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_test[numerical_cols_final] = numerical_imputer.transform(x_test[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_test[categorical_cols_final] = categorical_imputer.transform(x_test[categorical_cols_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfae23",
   "metadata": {},
   "source": [
    "# <u> Random Forest Model training <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f48739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa4fc5",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f1c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fold 1/3\n",
      "============================================================\n",
      "Train macro F1:    0.8620\n",
      "Train weighted F1: 0.8506\n",
      "Val macro F1:      0.5278\n",
      "Val weighted F1:   0.7401\n",
      "\n",
      "============================================================\n",
      "Fold 2/3\n",
      "============================================================\n",
      "Train macro F1:    0.8620\n",
      "Train weighted F1: 0.8506\n",
      "Val macro F1:      0.5278\n",
      "Val weighted F1:   0.7401\n",
      "\n",
      "============================================================\n",
      "Fold 2/3\n",
      "============================================================\n",
      "Train macro F1:    0.8667\n",
      "Train weighted F1: 0.8527\n",
      "Val macro F1:      0.5291\n",
      "Val weighted F1:   0.7405\n",
      "\n",
      "============================================================\n",
      "Fold 3/3\n",
      "============================================================\n",
      "Train macro F1:    0.8667\n",
      "Train weighted F1: 0.8527\n",
      "Val macro F1:      0.5291\n",
      "Val weighted F1:   0.7405\n",
      "\n",
      "============================================================\n",
      "Fold 3/3\n",
      "============================================================\n",
      "Train macro F1:    0.8656\n",
      "Train weighted F1: 0.8506\n",
      "Val macro F1:      0.5311\n",
      "Val weighted F1:   0.7395\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "Train macro   = 0.8647 ¬± 0.0020\n",
      "Train weighted= 0.8513 ¬± 0.0010\n",
      "Val   macro   = 0.5293 ¬± 0.0014\n",
      "Val   weighted= 0.7401 ¬± 0.0004\n",
      "Train macro F1:    0.8656\n",
      "Train weighted F1: 0.8506\n",
      "Val macro F1:      0.5311\n",
      "Val weighted F1:   0.7395\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "Train macro   = 0.8647 ¬± 0.0020\n",
      "Train weighted= 0.8513 ¬± 0.0010\n",
      "Val   macro   = 0.5293 ¬± 0.0014\n",
      "Val   weighted= 0.7401 ¬± 0.0004\n"
     ]
    }
   ],
   "source": [
    "X = x_train.values\n",
    "y = y_train.values\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "\n",
    "tr_macro_list, tr_weight_list = [], []\n",
    "va_macro_list, va_weight_list = [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y), start=1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/3\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "    # Random Forest model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    pred_tr = model.predict(X_tr)\n",
    "    pred_va = model.predict(X_va)\n",
    "\n",
    "    tr_macro = f1_score(y_tr, pred_tr, average=\"macro\")\n",
    "    tr_weight = f1_score(y_tr, pred_tr, average=\"weighted\")\n",
    "    va_macro = f1_score(y_va, pred_va, average=\"macro\")\n",
    "    va_weight = f1_score(y_va, pred_va, average=\"weighted\")\n",
    "\n",
    "    print(f\"Train macro F1:    {tr_macro:.4f}\")\n",
    "    print(f\"Train weighted F1: {tr_weight:.4f}\")\n",
    "    print(f\"Val macro F1:      {va_macro:.4f}\")\n",
    "    print(f\"Val weighted F1:   {va_weight:.4f}\")\n",
    "\n",
    "    tr_macro_list.append(tr_macro)\n",
    "    tr_weight_list.append(tr_weight)\n",
    "    va_macro_list.append(va_macro)\n",
    "    va_weight_list.append(va_weight)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train macro   = {np.mean(tr_macro_list):.4f} ¬± {np.std(tr_macro_list):.4f}\")\n",
    "print(f\"Train weighted= {np.mean(tr_weight_list):.4f} ¬± {np.std(tr_weight_list):.4f}\")\n",
    "print(f\"Val   macro   = {np.mean(va_macro_list):.4f} ¬± {np.std(va_macro_list):.4f}\")\n",
    "print(f\"Val   weighted= {np.mean(va_weight_list):.4f} ¬± {np.std(va_weight_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a77139b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  9.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:   22.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    4.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "============================================================\n",
      "Train macro F1:    0.9737\n",
      "Train weighted F1: 0.9723\n",
      "Val macro F1:      0.5254\n",
      "Val weighted F1:   0.7666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    5.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = x_train.values\n",
    "y = y_train.values\n",
    "\n",
    "# Simple train-test split\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=45,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "pred_tr = model.predict(X_tr)\n",
    "pred_va = model.predict(X_va)\n",
    "\n",
    "tr_macro = f1_score(y_tr, pred_tr, average=\"macro\")\n",
    "tr_weight = f1_score(y_tr, pred_tr, average=\"weighted\")\n",
    "va_macro = f1_score(y_va, pred_va, average=\"macro\")\n",
    "va_weight = f1_score(y_va, pred_va, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train macro F1:    {tr_macro:.4f}\")\n",
    "print(f\"Train weighted F1: {tr_weight:.4f}\")\n",
    "print(f\"Val macro F1:      {va_macro:.4f}\")\n",
    "print(f\"Val weighted F1:   {va_weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
