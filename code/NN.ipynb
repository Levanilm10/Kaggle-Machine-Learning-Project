{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5292d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587c072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "       'Mega Projects': 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f166e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyogrio/raw.py:200: RuntimeWarning: driver GeoJSON does not support open option INDEX_COL\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fa6c",
   "metadata": {},
   "source": [
    "# <u> Data Preprocessing and Cleaning <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['change_type'].map(change_type_map) #we convert the target variable to numeric values using the dictionary we created earlier\n",
    "x_train=train_df.drop(columns=['change_type','index'])#we drop index and change_type columns from the training data\n",
    "x_test=test_df.drop(columns=['index'])#we drop index column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3df0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee766b",
   "metadata": {},
   "source": [
    "### Geometry features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c87ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometry_features(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    gdf_m = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    area = gdf_m.geometry.area\n",
    "    perim = gdf_m.geometry.length\n",
    "\n",
    "    gdf_ll = gdf.to_crs(epsg=4326)\n",
    "    cent = gdf_ll.geometry.centroid\n",
    "    bounds = gdf_ll.geometry.bounds \n",
    "\n",
    "    gdf[\"geom_area_m2\"] = area\n",
    "    gdf[\"geom_perim_m\"] = perim\n",
    "    gdf[\"geom_compactness\"] = (4 * np.pi * area) / (perim**2 + 1e-9)\n",
    "\n",
    "    gdf[\"centroid_lon\"] = cent.x\n",
    "    gdf[\"centroid_lat\"] = cent.y\n",
    "\n",
    "    gdf[\"bbox_width_deg\"]  = bounds[\"maxx\"] - bounds[\"minx\"]\n",
    "    gdf[\"bbox_height_deg\"] = bounds[\"maxy\"] - bounds[\"miny\"]\n",
    "    gdf[\"bbox_area_deg2\"]  = gdf[\"bbox_width_deg\"] * gdf[\"bbox_height_deg\"]\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc06c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/shapely/measurement.py:50: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/shapely/measurement.py:196: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/shapely/measurement.py:50: RuntimeWarning: invalid value encountered in area\n",
      "  return lib.area(geometry, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/shapely/measurement.py:196: RuntimeWarning: invalid value encountered in length\n",
      "  return lib.length(geometry, **kwargs)\n",
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/2612037705.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  cent = gdf_ll.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "#for x_train\n",
    "x_train = add_geometry_features(x_train)\n",
    "x_train = x_train.drop(columns=[\"geometry\"])\n",
    "\n",
    "# for x_test\n",
    "x_test = add_geometry_features(x_test)\n",
    "x_test = x_test.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ff2e6",
   "metadata": {},
   "source": [
    "### Date features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6dab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _detect_date_indices(df):\n",
    "    date_cols = [c for c in df.columns if re.fullmatch(r\"date\\d+\", c)]\n",
    "    if not date_cols:\n",
    "        raise ValueError(\"Aucune colonne dateX trouvée (date0.. ou date1..)\")\n",
    "    idxs = sorted([int(c.replace(\"date\",\"\")) for c in date_cols])\n",
    "    return idxs, [f\"date{i}\" for i in idxs]\n",
    "\n",
    "def add_temporal_status_image_features(df: pd.DataFrame, drop_raw: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Trie chaque ligne par les vraies dates (NaT à la fin)\n",
    "    - Construit des features de statuts (ordre + transitions)\n",
    "    - Construit des features RGB (first/last/deltas) après tri\n",
    "    - Ajoute aussi date_span_days + deltas entre dates\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    idxs, date_cols = _detect_date_indices(df)\n",
    "    k = len(date_cols)\n",
    "\n",
    "    status_cols = [f\"change_status_date{i}\" for i in idxs if f\"change_status_date{i}\" in df.columns]\n",
    "    if len(status_cols) != k:\n",
    "\n",
    "        status_cols = [c for c in df.columns if re.fullmatch(r\"change_status_date\\d+\", c)]\n",
    "        status_idxs = sorted([int(c.split(\"date\")[-1]) for c in status_cols])\n",
    "        status_cols = [f\"change_status_date{i}\" for i in status_idxs]\n",
    "        k = min(len(date_cols), len(status_cols))\n",
    "        date_cols, status_cols, idxs = date_cols[:k], status_cols[:k], idxs[:k]\n",
    "\n",
    "    img_specs = []\n",
    "    for color in [\"red\", \"green\", \"blue\"]:\n",
    "        for stat in [\"mean\", \"std\"]:\n",
    "            cols = []\n",
    "            ok = True\n",
    "            for i in idxs[:k]:\n",
    "                c = f\"img_{color}_{stat}_date{i}\"\n",
    "                if c in df.columns:\n",
    "                    cols.append(c)\n",
    "                else:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                img_specs.append((color, stat, cols))\n",
    "\n",
    "\n",
    "    D = df[date_cols].apply(pd.to_datetime, dayfirst=True, errors=\"coerce\").to_numpy(dtype=\"datetime64[ns]\")\n",
    "    Di = D.astype(\"int64\")\n",
    "    nat_mask = (Di == np.iinfo(np.int64).min)\n",
    "    Di[nat_mask] = np.iinfo(np.int64).max\n",
    "\n",
    "    order = np.argsort(Di, axis=1)\n",
    "    Di_sorted = np.take_along_axis(Di, order, axis=1)\n",
    "\n",
    "    maxv = np.iinfo(np.int64).max\n",
    "    valid_mask = (Di_sorted != maxv)\n",
    "    last_idx = valid_mask.sum(axis=1) - 1\n",
    "\n",
    "\n",
    "    span_days = np.full(len(df), np.nan)\n",
    "    ok = last_idx >= 0\n",
    "    span_days[ok] = (Di_sorted[ok, last_idx[ok]] - Di_sorted[ok, 0]) / (24 * 3600 * 1e9)\n",
    "    df[\"date_span_days\"] = span_days\n",
    "\n",
    "    for j in range(k - 1):\n",
    "        a, b = Di_sorted[:, j], Di_sorted[:, j + 1]\n",
    "        delta = (b - a) / (24 * 3600 * 1e9)\n",
    "        bad = (a == maxv) | (b == maxv)\n",
    "        delta[bad] = np.nan\n",
    "        df[f\"delta_sorted_{j}_{j+1}_days\"] = delta\n",
    "\n",
    "\n",
    "    S = df[status_cols].astype(object).to_numpy()\n",
    "    S_sorted = np.take_along_axis(S, order[:, :k], axis=1)\n",
    "\n",
    "\n",
    "    Sdf = pd.DataFrame(S_sorted)\n",
    "    df[\"status_first\"] = Sdf.bfill(axis=1).iloc[:, 0]\n",
    "    df[\"status_last\"]  = Sdf.ffill(axis=1).iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_first\"], prefix=\"status_first\", dummy_na=True)], axis=1)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"status_last\"],  prefix=\"status_last\",  dummy_na=True)], axis=1)\n",
    "\n",
    "\n",
    "    level_map = {\n",
    "        \"Land Cleared\": 0,\n",
    "        \"Construction Started\": 1,\n",
    "        \"Materials Dumped\": 1,\n",
    "        \"Construction Midway\": 2,\n",
    "        \"Construction Done\": 3,\n",
    "        \"Prior Construction\": 3,\n",
    "        \"Operational\": 4,\n",
    "    }\n",
    "\n",
    "    L = np.full((len(df), k), np.nan)\n",
    "    for j in range(k):\n",
    "        L[:, j] = pd.Series(S_sorted[:, j]).map(level_map).to_numpy(dtype=float)\n",
    "        df[f\"status_level_sorted_{j}\"] = L[:, j]\n",
    "\n",
    "    Ldf = pd.DataFrame(L)\n",
    "    first_level = Ldf.bfill(axis=1).iloc[:, 0].to_numpy()\n",
    "    last_level  = Ldf.ffill(axis=1).iloc[:, -1].to_numpy()\n",
    "\n",
    "    df[\"status_level_first\"] = first_level\n",
    "    df[\"status_level_last\"]  = last_level\n",
    "    df[\"status_progress\"]    = last_level - first_level\n",
    "    df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
    "    df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
    "\n",
    "\n",
    "    mask = ~pd.DataFrame(S_sorted).isna().to_numpy()\n",
    "    n_changes = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        m = mask[:, j] & mask[:, j + 1]\n",
    "        n_changes += (m & (S_sorted[:, j] != S_sorted[:, j + 1]))\n",
    "    df[\"status_n_changes\"] = n_changes\n",
    "\n",
    "    n_reg = np.zeros(len(df), dtype=float)\n",
    "    for j in range(k - 1):\n",
    "        a, b = L[:, j], L[:, j + 1]\n",
    "        m = np.isfinite(a) & np.isfinite(b)\n",
    "        n_reg += (m & (b < a))\n",
    "    df[\"status_n_regressions\"] = n_reg\n",
    "    df[\"status_has_regression\"] = (n_reg > 0).astype(int)\n",
    "\n",
    "\n",
    "    first_date = Di_sorted[:, 0]\n",
    "    time_to_done = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)):\n",
    "        if first_date[i] == maxv:\n",
    "            continue\n",
    "        idx_done = None\n",
    "        for j in range(k):\n",
    "            if valid_mask[i, j] and np.isfinite(L[i, j]) and L[i, j] >= 3:\n",
    "                idx_done = j\n",
    "                break\n",
    "        if idx_done is not None:\n",
    "            time_to_done[i] = (Di_sorted[i, idx_done] - first_date[i]) / (24 * 3600 * 1e9)\n",
    "    df[\"time_to_done_days\"] = time_to_done\n",
    "\n",
    "\n",
    "    for color, stat, cols in img_specs:\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        Xs = np.take_along_axis(X, order[:, :k], axis=1)\n",
    "        Xs = Xs.copy()\n",
    "        Xs[~valid_mask] = np.nan\n",
    "\n",
    "        first = Xs[:, 0]\n",
    "        last = np.full(len(df), np.nan)\n",
    "        ok = last_idx >= 0\n",
    "        last[ok] = Xs[ok, last_idx[ok]]\n",
    "\n",
    "        df[f\"img_{color}_{stat}_first\"] = first\n",
    "        df[f\"img_{color}_{stat}_last\"]  = last\n",
    "        df[f\"img_{color}_{stat}_delta\"] = last - first\n",
    "\n",
    "        for j in range(k - 1):\n",
    "            df[f\"img_{color}_{stat}_delta_{j}_{j+1}\"] = Xs[:, j + 1] - Xs[:, j]\n",
    "\n",
    "\n",
    "    if drop_raw:\n",
    "        drop_cols = []\n",
    "        drop_cols += date_cols + status_cols\n",
    "        for _, _, cols in img_specs:\n",
    "            drop_cols += cols\n",
    "        drop_cols += [\"status_first\", \"status_last\"]\n",
    "        df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622d8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n",
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/754418011.py:107: RuntimeWarning: All-NaN slice encountered\n",
      "  df[\"status_level_max\"]   = np.nanmax(L, axis=1)\n",
      "/var/folders/52/gw_8zl6n65b05xp9tk246k2m0000gn/T/ipykernel_61399/754418011.py:108: RuntimeWarning: Mean of empty slice\n",
      "  df[\"status_level_mean\"]  = np.nanmean(L, axis=1)\n"
     ]
    }
   ],
   "source": [
    "#create temporal features for both train and test\n",
    "x_train = add_temporal_status_image_features(x_train)\n",
    "x_test = add_temporal_status_image_features(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4ef1e",
   "metadata": {},
   "source": [
    "### Multi-hot encoding: urban_type & geography_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab1b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_encode(df: pd.DataFrame, column: str, all_values: set = None) -> tuple:\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "\n",
    "    if all_values is None:\n",
    "        all_values = set()\n",
    "        for val in df[column].dropna():\n",
    "            if pd.notna(val) and val != 'N,A':  \n",
    "\n",
    "                values = [v.strip() for v in str(val).split(',')]\n",
    "                all_values.update(values)\n",
    "    \n",
    "\n",
    "    for value in sorted(all_values):\n",
    "\n",
    "        col_name = f\"{column}_{value.replace(' ', '_').replace('/', '_')}\"\n",
    "        \n",
    "\n",
    "        df[col_name] = df[column].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x).split(',') else 0\n",
    "        )\n",
    "    \n",
    "\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df, all_values\n",
    "\n",
    "\n",
    "\n",
    "def extract_all_values(df: pd.DataFrame, column: str) -> set:\n",
    "    \"\"\"Extrait toutes les valeurs uniques d'une colonne.\"\"\"\n",
    "    all_values = set()\n",
    "    for val in df[column].dropna():\n",
    "        if pd.notna(val) and val != 'N,A':\n",
    "            values = [v.strip() for v in str(val).split(',')]\n",
    "            all_values.update(values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "urban_values_train = extract_all_values(x_train, 'urban_type')\n",
    "urban_values_test = extract_all_values(test_df, 'urban_type')\n",
    "urban_values_all = urban_values_train | urban_values_test  \n",
    "\n",
    "geo_values_train = extract_all_values(x_train, 'geography_type')\n",
    "geo_values_test = extract_all_values(test_df, 'geography_type')\n",
    "geo_values_all = geo_values_train | geo_values_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3301c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (296146, 95)\n",
      "Test shape: (120526, 95)\n"
     ]
    }
   ],
   "source": [
    "#multi-hot encoding on train\n",
    "x_train, _ = multi_hot_encode(x_train, 'urban_type', urban_values_all)\n",
    "x_train, _ = multi_hot_encode(x_train, 'geography_type', geo_values_all)\n",
    "print(f\"Train shape: {x_train.shape}\")\n",
    "\n",
    "#multi-hot encoding on test\n",
    "x_test, _ = multi_hot_encode(x_test, 'urban_type', urban_values_all)\n",
    "x_test, _ = multi_hot_encode(x_test, 'geography_type', geo_values_all)\n",
    "print(f\"Test shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0e146",
   "metadata": {},
   "source": [
    "### Drop additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8c664b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"delta_sorted_0_1_days\",\"delta_sorted_1_2_days\", \"delta_sorted_2_3_days\", \"delta_sorted_3_4_days\",\"index\"]\n",
    "\n",
    "\n",
    "cols_to_drop_train = [col for col in cols_to_drop if col in x_train.columns]\n",
    "cols_to_drop_test = [col for col in cols_to_drop if col in x_test.columns]\n",
    "\n",
    "x_train = x_train.drop(columns=cols_to_drop_train)\n",
    "x_test = x_test.drop(columns=cols_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec455e",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f128af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 infinity values\n",
      "Test: 2 infinity values\n"
     ]
    }
   ],
   "source": [
    "inf_count_train = np.isinf(x_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_count_test = np.isinf(x_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Train: {inf_count_train} infinity values\")\n",
    "print(f\"Test: {inf_count_test} infinity values\")\n",
    "\n",
    "# Replace inf values with NaN for imputation\n",
    "x_train = x_train.replace([np.inf, -np.inf], np.nan)\n",
    "x_test = x_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_final = x_train.select_dtypes(include=[\"number\", \"bool\"]).columns.to_list()\n",
    "categorical_cols_final = x_train.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute x_train\n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_train[numerical_cols_final] = numerical_imputer.fit_transform(x_train[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_train[categorical_cols_final] = categorical_imputer.fit_transform(x_train[categorical_cols_final])\n",
    "\n",
    "# Impute x_test \n",
    "if len(numerical_cols_final) > 0:\n",
    "    x_test[numerical_cols_final] = numerical_imputer.transform(x_test[numerical_cols_final])\n",
    "if len(categorical_cols_final) > 0:\n",
    "    x_test[categorical_cols_final] = categorical_imputer.transform(x_test[categorical_cols_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c49bd",
   "metadata": {},
   "source": [
    "# <u> NeuralNetwork Modele training <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "315b0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b057167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "X = x_train.values.astype(np.float32)\n",
    "y = y_train.values.astype(np.int64)\n",
    "\n",
    "# Define dimensions\n",
    "in_dim = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, va_idx = next(sss.split(X, y))\n",
    "X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
    "X_va = scaler.transform(X_va).astype(np.float32)\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "val_ds   = TensorDataset(torch.from_numpy(X_va), torch.from_numpy(y_va))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2048, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4096, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "classes = np.unique(y_tr)\n",
    "w = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_w = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "def eval_f1(model, loader):\n",
    "    model.eval()\n",
    "    all_pred, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_pred.append(pred)\n",
    "            all_true.append(yb.numpy())\n",
    "    yhat = np.concatenate(all_pred)\n",
    "    ytrue = np.concatenate(all_true)\n",
    "    wF1 = f1_score(ytrue, yhat, average=\"weighted\")\n",
    "    mF1 = f1_score(ytrue, yhat, average=\"macro\")\n",
    "    return wF1, mF1\n",
    "\n",
    "def train_with_early_stopping(model, optimizer, criterion,\n",
    "                              max_epochs=50, patience=5, min_delta=1e-4):\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        tr_w, tr_m = eval_f1(model, train_loader)\n",
    "        va_w, va_m = eval_f1(model, val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train wF1={tr_w:.4f} mF1={tr_m:.4f} | \"\n",
    "            f\"Val wF1={va_w:.4f} mF1={va_m:.4f}\"\n",
    "        )\n",
    "\n",
    "        if va_w > best_val + min_delta:\n",
    "            best_val = va_w\n",
    "            bad = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    best_tr_w, best_tr_m = eval_f1(model, train_loader)\n",
    "    best_va_w, best_va_m = eval_f1(model, val_loader)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"best_train_wF1\": best_tr_w,\n",
    "        \"best_train_mF1\": best_tr_m,\n",
    "        \"best_val_wF1\": best_va_w,\n",
    "        \"best_val_mF1\": best_va_m,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde25d4",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de32afe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class MLP1(nn.Module):\\n    def __init__(self, in_dim, n_classes):\\n        super().__init__()\\n        self.net = nn.Sequential(\\n            nn.Linear(in_dim, 256),\\n            nn.BatchNorm1d(256),\\n            nn.ReLU(),\\n            nn.Dropout(0.25),\\n\\n            nn.Linear(256, 128),\\n            nn.BatchNorm1d(128),\\n            nn.ReLU(),\\n            nn.Dropout(0.25),\\n\\n            nn.Linear(128, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, n_classes),\\n        )\\n\\n    def forward(self, x):\\n        return self.net(x)\\n\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"TRAINING MODEL 1 (baseline)\")\\nprint(\"=\"*70)\\n\\nmodel1 = MLP1(in_dim=in_dim, n_classes=n_classes).to(device)\\ncriterion1 = nn.CrossEntropyLoss(weight=class_w.to(device))\\noptimizer1 = torch.optim.AdamW(model1.parameters(), lr=1e-3, weight_decay=1e-4)\\n\\nres1 = train_with_early_stopping(\\n    model=model1,\\n    optimizer=optimizer1,\\n    criterion=criterion1,\\n    max_epochs=50,\\n    patience=5,\\n    min_delta=1e-4\\n)\\n\\nprint(\"\\nMODEL 1 BEST SCORES\")\\nprint(\"Best TRAIN wF1:\", res1[\"best_train_wF1\"], \"| Best TRAIN mF1:\", res1[\"best_train_mF1\"])\\nprint(\"Best VAL   wF1:\", res1[\"best_val_wF1\"],   \"| Best VAL   mF1:\", res1[\"best_val_mF1\"])\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class MLP1(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODEL 1 (baseline)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model1 = MLP1(in_dim=in_dim, n_classes=n_classes).to(device)\n",
    "criterion1 = nn.CrossEntropyLoss(weight=class_w.to(device))\n",
    "optimizer1 = torch.optim.AdamW(model1.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "res1 = train_with_early_stopping(\n",
    "    model=model1,\n",
    "    optimizer=optimizer1,\n",
    "    criterion=criterion1,\n",
    "    max_epochs=50,\n",
    "    patience=5,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nMODEL 1 BEST SCORES\")\n",
    "print(\"Best TRAIN wF1:\", res1[\"best_train_wF1\"], \"| Best TRAIN mF1:\", res1[\"best_train_mF1\"])\n",
    "print(\"Best VAL   wF1:\", res1[\"best_val_wF1\"],   \"| Best VAL   mF1:\", res1[\"best_val_mF1\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69795a41",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c588c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING MODEL 2 (higher capacity / less regularization)\n",
      "======================================================================\n",
      "Epoch 01 | Train wF1=0.6222 mF1=0.4341 | Val wF1=0.6212 mF1=0.4319\n",
      "Epoch 01 | Train wF1=0.6222 mF1=0.4341 | Val wF1=0.6212 mF1=0.4319\n",
      "Epoch 02 | Train wF1=0.6419 mF1=0.4432 | Val wF1=0.6396 mF1=0.4389\n",
      "Epoch 02 | Train wF1=0.6419 mF1=0.4432 | Val wF1=0.6396 mF1=0.4389\n",
      "Epoch 03 | Train wF1=0.5992 mF1=0.4224 | Val wF1=0.5981 mF1=0.4206\n",
      "Epoch 03 | Train wF1=0.5992 mF1=0.4224 | Val wF1=0.5981 mF1=0.4206\n",
      "Epoch 04 | Train wF1=0.6473 mF1=0.4519 | Val wF1=0.6443 mF1=0.4456\n",
      "Epoch 04 | Train wF1=0.6473 mF1=0.4519 | Val wF1=0.6443 mF1=0.4456\n",
      "Epoch 05 | Train wF1=0.6210 mF1=0.4293 | Val wF1=0.6203 mF1=0.4252\n",
      "Epoch 05 | Train wF1=0.6210 mF1=0.4293 | Val wF1=0.6203 mF1=0.4252\n",
      "Epoch 06 | Train wF1=0.6304 mF1=0.4336 | Val wF1=0.6285 mF1=0.4275\n",
      "Epoch 06 | Train wF1=0.6304 mF1=0.4336 | Val wF1=0.6285 mF1=0.4275\n",
      "Epoch 07 | Train wF1=0.6368 mF1=0.4489 | Val wF1=0.6319 mF1=0.4412\n",
      "Epoch 07 | Train wF1=0.6368 mF1=0.4489 | Val wF1=0.6319 mF1=0.4412\n",
      "Epoch 08 | Train wF1=0.6545 mF1=0.4534 | Val wF1=0.6512 mF1=0.4442\n",
      "Epoch 08 | Train wF1=0.6545 mF1=0.4534 | Val wF1=0.6512 mF1=0.4442\n",
      "Epoch 09 | Train wF1=0.6678 mF1=0.4662 | Val wF1=0.6626 mF1=0.4535\n",
      "Epoch 09 | Train wF1=0.6678 mF1=0.4662 | Val wF1=0.6626 mF1=0.4535\n",
      "Epoch 10 | Train wF1=0.6444 mF1=0.4475 | Val wF1=0.6422 mF1=0.4381\n",
      "Epoch 10 | Train wF1=0.6444 mF1=0.4475 | Val wF1=0.6422 mF1=0.4381\n",
      "Epoch 11 | Train wF1=0.6478 mF1=0.4503 | Val wF1=0.6439 mF1=0.4386\n",
      "Epoch 11 | Train wF1=0.6478 mF1=0.4503 | Val wF1=0.6439 mF1=0.4386\n",
      "Epoch 12 | Train wF1=0.6563 mF1=0.4572 | Val wF1=0.6519 mF1=0.4459\n",
      "Epoch 12 | Train wF1=0.6563 mF1=0.4572 | Val wF1=0.6519 mF1=0.4459\n",
      "Epoch 13 | Train wF1=0.6709 mF1=0.4701 | Val wF1=0.6685 mF1=0.4544\n",
      "Epoch 13 | Train wF1=0.6709 mF1=0.4701 | Val wF1=0.6685 mF1=0.4544\n",
      "Epoch 14 | Train wF1=0.6655 mF1=0.4689 | Val wF1=0.6628 mF1=0.4546\n",
      "Epoch 14 | Train wF1=0.6655 mF1=0.4689 | Val wF1=0.6628 mF1=0.4546\n",
      "Epoch 15 | Train wF1=0.6764 mF1=0.4751 | Val wF1=0.6725 mF1=0.4578\n",
      "Epoch 15 | Train wF1=0.6764 mF1=0.4751 | Val wF1=0.6725 mF1=0.4578\n",
      "Epoch 16 | Train wF1=0.6723 mF1=0.4893 | Val wF1=0.6677 mF1=0.4571\n",
      "Epoch 16 | Train wF1=0.6723 mF1=0.4893 | Val wF1=0.6677 mF1=0.4571\n",
      "Epoch 17 | Train wF1=0.6823 mF1=0.4975 | Val wF1=0.6776 mF1=0.4640\n",
      "Epoch 17 | Train wF1=0.6823 mF1=0.4975 | Val wF1=0.6776 mF1=0.4640\n",
      "Epoch 18 | Train wF1=0.6870 mF1=0.5013 | Val wF1=0.6816 mF1=0.4691\n",
      "Epoch 18 | Train wF1=0.6870 mF1=0.5013 | Val wF1=0.6816 mF1=0.4691\n",
      "Epoch 19 | Train wF1=0.6913 mF1=0.5105 | Val wF1=0.6859 mF1=0.4692\n",
      "Epoch 19 | Train wF1=0.6913 mF1=0.5105 | Val wF1=0.6859 mF1=0.4692\n",
      "Epoch 20 | Train wF1=0.6861 mF1=0.4998 | Val wF1=0.6798 mF1=0.4681\n",
      "Epoch 20 | Train wF1=0.6861 mF1=0.4998 | Val wF1=0.6798 mF1=0.4681\n",
      "Epoch 21 | Train wF1=0.6925 mF1=0.5154 | Val wF1=0.6859 mF1=0.4706\n",
      "Epoch 21 | Train wF1=0.6925 mF1=0.5154 | Val wF1=0.6859 mF1=0.4706\n",
      "Epoch 22 | Train wF1=0.6783 mF1=0.4823 | Val wF1=0.6732 mF1=0.4589\n",
      "Epoch 22 | Train wF1=0.6783 mF1=0.4823 | Val wF1=0.6732 mF1=0.4589\n",
      "Epoch 23 | Train wF1=0.6859 mF1=0.4895 | Val wF1=0.6790 mF1=0.4643\n",
      "Epoch 23 | Train wF1=0.6859 mF1=0.4895 | Val wF1=0.6790 mF1=0.4643\n",
      "Epoch 24 | Train wF1=0.6882 mF1=0.4914 | Val wF1=0.6814 mF1=0.4667\n",
      "Epoch 24 | Train wF1=0.6882 mF1=0.4914 | Val wF1=0.6814 mF1=0.4667\n",
      "Epoch 25 | Train wF1=0.6788 mF1=0.4820 | Val wF1=0.6725 mF1=0.4591\n",
      "Epoch 25 | Train wF1=0.6788 mF1=0.4820 | Val wF1=0.6725 mF1=0.4591\n",
      "Epoch 26 | Train wF1=0.6844 mF1=0.4852 | Val wF1=0.6799 mF1=0.4665\n",
      "Early stopping.\n",
      "Epoch 26 | Train wF1=0.6844 mF1=0.4852 | Val wF1=0.6799 mF1=0.4665\n",
      "Early stopping.\n",
      "\n",
      "MODEL 2 BEST SCORES\n",
      "Best TRAIN wF1: 0.6913238325829032 | Best TRAIN mF1: 0.510459920898185\n",
      "Best VAL   wF1: 0.6858651596772688 | Best VAL   mF1: 0.4691520606844934\n",
      "\n",
      "MODEL 2 BEST SCORES\n",
      "Best TRAIN wF1: 0.6913238325829032 | Best TRAIN mF1: 0.510459920898185\n",
      "Best VAL   wF1: 0.6858651596772688 | Best VAL   mF1: 0.4691520606844934\n"
     ]
    }
   ],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, in_dim, n_classes, dropout=0.10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MODEL 2 (higher capacity / less regularization)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model2 = MLP2(in_dim=in_dim, n_classes=n_classes, dropout=0.10).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss(weight=class_w.to(device))\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "res2 = train_with_early_stopping(\n",
    "    model=model2,\n",
    "    optimizer=optimizer2,\n",
    "    criterion=criterion2,\n",
    "    max_epochs=80,\n",
    "    patience=7,\n",
    "    min_delta=1e-4\n",
    ")\n",
    "\n",
    "print(\"\\nMODEL 2 BEST SCORES\")\n",
    "print(\"Best TRAIN wF1:\", res2[\"best_train_wF1\"], \"| Best TRAIN mF1:\", res2[\"best_train_mF1\"])\n",
    "print(\"Best VAL   wF1:\", res2[\"best_val_wF1\"],   \"| Best VAL   mF1:\", res2[\"best_val_mF1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
